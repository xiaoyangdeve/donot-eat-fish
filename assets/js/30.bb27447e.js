(window.webpackJsonp=window.webpackJsonp||[]).push([[30],{346:function(t,s,a){"use strict";a.r(s);var n=a(1),r=Object(n.a)({},(function(){var t=this,s=t._self._c;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h2",{attrs:{id:"_1-时间序列介绍"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-时间序列介绍"}},[t._v("#")]),t._v(" 1.时间序列介绍")]),t._v(" "),s("p",[s("strong",[t._v("时间序列")]),t._v("（英语：time series）是一组按照时间发生先后顺序进行排列的数据点。通常一组时间序列的时间间隔为一恒定值（如1秒，5分钟，12小时，7天，1年，股票每天的收盘价等），因此时间序列可以作为离散时间数据进行分析处理。")]),t._v(" "),s("h3",{attrs:{id:"时间序列变量的特征"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#时间序列变量的特征"}},[t._v("#")]),t._v(" 时间序列变量的特征")]),t._v(" "),s("ul",[s("li",[s("strong",[t._v("非平稳性")]),t._v("（nonstationarity，也译作"),s("strong",[t._v("不平稳性")]),t._v("，"),s("strong",[t._v("非稳定性")]),t._v("）：即时间序列的方差无法呈现出一个长期趋势并最终趋于一个常数或是一个线性函数")]),t._v(" "),s("li",[t._v("波动幅度"),s("strong",[t._v("随时间变化")]),t._v("（Time－varying Volatility）：即一个时间序列变量的方差随时间的变化而变化")])]),t._v(" "),s("p",[t._v("这两个特征使得有效分析时间序列变量十分困难。")]),t._v(" "),s("p",[t._v("平稳型时间数列（Stationary Time Series）系指一个时间数列其统计特性将不随时间之变化而改变者。")]),t._v(" "),s("p",[s("img",{attrs:{src:"/donot-eat-fish/img/data_analysis/time_series//white_notise.png",alt:"白噪声"}}),s("img",{attrs:{src:"/donot-eat-fish/img/data_analysis/time_series//random_walk.png",alt:"随机游走"}})]),t._v(" "),s("center",[t._v("前者白噪声序列是常见的平稳序列，后者随即游走是分平稳序列。")]),t._v("\n时间序列数据变动存在着规律性与不规律性\n"),s("p",[t._v("时间序列中的每个观察值大小,是影响变化的各种不同因素在同一时刻发生作用的综合结果。从这些影响因素发生作用的大小和方向变化的时间特性来看,这些因素造成的时间序列数据的变动分为四种类型。")]),t._v(" "),s("ol",[s("li",[s("p",[t._v("趋势性(T):某个变量随着时间进展或自变量变化,呈现一种比较缓慢而长期的持续上升、下降、停留的同性质变动趋向,但变动幅度可能不相等。")])]),t._v(" "),s("li",[s("p",[t._v("周期性(S):某因素由于外部影响随着自然季节的交替出现高峰与低谷的规律。")])]),t._v(" "),s("li",[s("p",[t._v("随机性(R):个别为随机变动,整体呈统计规律。")])]),t._v(" "),s("li",[s("p",[t._v("综合性:实际变化情况是几种变动的叠加或组合。预测时设法过滤除去不规则变动,突出反映趋势性和周期性变动")])])]),t._v(" "),s("p",[t._v("从实际的序列中把各个变动的部分分离出来是对时间序列分析的首要任务，将分离得到的部分进行分析后再通过函数的变化实现 对总体序列的分析")]),t._v(" "),s("h3",{attrs:{id:"时间序列的平稳化"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#时间序列的平稳化"}},[t._v("#")]),t._v(" 时间序列的平稳化")]),t._v(" "),s("p",[t._v("在熟悉了平稳性的概念及其不同的类型之后，接下来可以对序列进行平稳化操作。请记住，为了建立时间序列预测模型，必须首先将任何非平稳序列转换为平稳序列。常用于平稳化的手段有以下一些：")]),t._v(" "),s("ul",[s("li",[t._v("差分化")]),t._v(" "),s("li",[t._v("季节性差分")]),t._v(" "),s("li",[t._v("变换：幂变换、平方根变换、对数变换")])]),t._v(" "),s("h2",{attrs:{id:"_2-短期预测"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-短期预测"}},[t._v("#")]),t._v(" 2.短期预测")]),t._v(" "),s("p",[t._v("在团队大牛的建议下，选择了日活、日总使用时长、日人均使用时长三份样本数据进行建模分析。同时使用Python构建三个不同的模型，并检查其结果。")]),t._v(" "),s("ul",[s("li",[t._v("ARIMA：差分整合移动平均自回归模型")]),t._v(" "),s("li",[t._v("LSTM：长短期记忆神经网络")]),t._v(" "),s("li",[t._v("Facebook Prophet：Facebook公司开源的时间序列预测库的")])]),t._v(" "),s("h3",{attrs:{id:"arima"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#arima"}},[t._v("#")]),t._v(" ARIMA")]),t._v(" "),s("p",[t._v("ARIMA是一个用于预测未来趋势的时间序列数据模型。模型是回归分析的一种形式。")]),t._v(" "),s("ul",[s("li",[t._v("AR（Autoregression）：显示变量变化的模型，该变量在其自身的滞后/先验值上回归。")]),t._v(" "),s("li",[t._v("I（Integrated）：差分时间序列的原始观测数据,使其平稳")]),t._v(" "),s("li",[t._v("MA（Moving average）：观察值与移动平均模型的残差之间的依赖关系")])]),t._v(" "),s("p",[t._v("对于ARIMA模型，标准的表示法是带有p、d和q的ARIMA，其中整数值替代参数来表示所使用的ARIMA模型的类型。")]),t._v(" "),s("ul",[s("li",[t._v("p：自回归阶数")]),t._v(" "),s("li",[t._v("d：差分次数")]),t._v(" "),s("li",[t._v("q：移动平均阶数")])]),t._v(" "),s("h3",{attrs:{id:"lstm神经网路"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#lstm神经网路"}},[t._v("#")]),t._v(" LSTM神经网路")]),t._v(" "),s("p",[t._v("LSTM是RNN的一种特殊类型，可以学习长期依赖信息，将长期记忆引入循环神经网络，缓解了梯度消失问题。它通过一系列“门”（Input Gate、Output Gate、Forget Gate）来实现这一点。")]),t._v(" "),s("h3",{attrs:{id:"prophet"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#prophet"}},[t._v("#")]),t._v(" Prophet")]),t._v(" "),s("p",[t._v("Prophet是一种基于加法模型预测时间序列数据的过程，其中非线性趋势与年、周、日季节性以及假日效应相吻合。它最适用于具有强烈季节效应和几个季节的历史数据的时间序列。Prophet对缺失的数据和趋势的变化是健壮的，通常能很好地处理异常值。")]),t._v(" "),s("p",[t._v("下图为使用同一份日活数据用以上三种方式简单建模，预测了未来30天的数据与实际值的对比。")]),t._v(" "),s("p",[s("img",{attrs:{src:"/donot-eat-fish/img/data_analysis/time_series//compare_lstm_arima_prophet.png",alt:"image-20191205152416970"}})]),t._v(" "),s("p",[t._v("通过对比可以看出prophet的预测更接近实际值一些，下面将使用prophet对三份样本进行建模和预测。主要步骤有：")]),t._v(" "),s("ul",[s("li",[t._v("1、读取数据，对数据进行平稳性检查，并根据结果进行差分和移动平均及对数化")]),t._v(" "),s("li",[t._v("2、添加模型参数、假日因子，建立模型并进行短期预测")]),t._v(" "),s("li",[t._v("3、对预测结果进行检查评估")])]),t._v(" "),s("p",[t._v("附主要部分代码：")]),t._v(" "),s("div",{staticClass:"language-python line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("build_model_predict")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                        input_df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                        cap"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                        floor"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                        period"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("7")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" holidays_country"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" freq"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'D'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n        构建模型 预测\n        :parameter input_df 模型输入数据\n        :parameter holidays: 假日df\n        :parameter holidays_country: 需要添加的假日国家\n        :parameter cap: logistic上限容量\n        :parameter floor: logistic下限容量\n        :parameter period: 预测天数\n        :parameter freq 频率，默认D\n\n        :return forecast\n    """')]),t._v("\n\n    logger"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("info"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'开始prophet建模预测{growth=%s, seasonality_model=%s, cap_threshold=%.4f}'")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("growth"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("seasonality_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cap_threshold"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 构建模型")]),t._v("\n    m "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Prophet"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    m"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("input_df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    future "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" m"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("make_future_dataframe"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("period"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" freq"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("freq"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 构建预测模型")]),t._v("\n    predict_model "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Prophet"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n        holidays"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("holidays"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        seasonality_mode"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("seasonality_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        growth"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("growth\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 添加假日")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" holidays_country "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("is")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        predict_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add_country_holidays"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("country_name"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("holidays_country"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 设定序列上下容量")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("growth "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'logistic'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" cap "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("is")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            cap "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("max")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("input_df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'y'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cap_threshold\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" floor "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("is")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            y_min "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("min")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("input_df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'y'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            cap_threshold_inner "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cap_threshold\n            "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cap_threshold "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                cap_threshold_inner "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cap_threshold\n            "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" y_min "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                floor "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" y_min "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" cap_threshold_inner\n            "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                floor "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" y_min "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" cap_threshold_inner\n        input_df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'cap'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" cap\n        input_df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'floor'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" floor\n        future"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'cap'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" cap\n        future"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'floor'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" floor\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 模型拟合及预测")]),t._v("\n    forecast "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" predict_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("input_df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("future"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" forecast\n")])]),t._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[t._v("1")]),s("br"),s("span",{staticClass:"line-number"},[t._v("2")]),s("br"),s("span",{staticClass:"line-number"},[t._v("3")]),s("br"),s("span",{staticClass:"line-number"},[t._v("4")]),s("br"),s("span",{staticClass:"line-number"},[t._v("5")]),s("br"),s("span",{staticClass:"line-number"},[t._v("6")]),s("br"),s("span",{staticClass:"line-number"},[t._v("7")]),s("br"),s("span",{staticClass:"line-number"},[t._v("8")]),s("br"),s("span",{staticClass:"line-number"},[t._v("9")]),s("br"),s("span",{staticClass:"line-number"},[t._v("10")]),s("br"),s("span",{staticClass:"line-number"},[t._v("11")]),s("br"),s("span",{staticClass:"line-number"},[t._v("12")]),s("br"),s("span",{staticClass:"line-number"},[t._v("13")]),s("br"),s("span",{staticClass:"line-number"},[t._v("14")]),s("br"),s("span",{staticClass:"line-number"},[t._v("15")]),s("br"),s("span",{staticClass:"line-number"},[t._v("16")]),s("br"),s("span",{staticClass:"line-number"},[t._v("17")]),s("br"),s("span",{staticClass:"line-number"},[t._v("18")]),s("br"),s("span",{staticClass:"line-number"},[t._v("19")]),s("br"),s("span",{staticClass:"line-number"},[t._v("20")]),s("br"),s("span",{staticClass:"line-number"},[t._v("21")]),s("br"),s("span",{staticClass:"line-number"},[t._v("22")]),s("br"),s("span",{staticClass:"line-number"},[t._v("23")]),s("br"),s("span",{staticClass:"line-number"},[t._v("24")]),s("br"),s("span",{staticClass:"line-number"},[t._v("25")]),s("br"),s("span",{staticClass:"line-number"},[t._v("26")]),s("br"),s("span",{staticClass:"line-number"},[t._v("27")]),s("br"),s("span",{staticClass:"line-number"},[t._v("28")]),s("br"),s("span",{staticClass:"line-number"},[t._v("29")]),s("br"),s("span",{staticClass:"line-number"},[t._v("30")]),s("br"),s("span",{staticClass:"line-number"},[t._v("31")]),s("br"),s("span",{staticClass:"line-number"},[t._v("32")]),s("br"),s("span",{staticClass:"line-number"},[t._v("33")]),s("br"),s("span",{staticClass:"line-number"},[t._v("34")]),s("br"),s("span",{staticClass:"line-number"},[t._v("35")]),s("br"),s("span",{staticClass:"line-number"},[t._v("36")]),s("br"),s("span",{staticClass:"line-number"},[t._v("37")]),s("br"),s("span",{staticClass:"line-number"},[t._v("38")]),s("br"),s("span",{staticClass:"line-number"},[t._v("39")]),s("br"),s("span",{staticClass:"line-number"},[t._v("40")]),s("br"),s("span",{staticClass:"line-number"},[t._v("41")]),s("br"),s("span",{staticClass:"line-number"},[t._v("42")]),s("br"),s("span",{staticClass:"line-number"},[t._v("43")]),s("br"),s("span",{staticClass:"line-number"},[t._v("44")]),s("br"),s("span",{staticClass:"line-number"},[t._v("45")]),s("br"),s("span",{staticClass:"line-number"},[t._v("46")]),s("br"),s("span",{staticClass:"line-number"},[t._v("47")]),s("br"),s("span",{staticClass:"line-number"},[t._v("48")]),s("br"),s("span",{staticClass:"line-number"},[t._v("49")]),s("br"),s("span",{staticClass:"line-number"},[t._v("50")]),s("br"),s("span",{staticClass:"line-number"},[t._v("51")]),s("br"),s("span",{staticClass:"line-number"},[t._v("52")]),s("br"),s("span",{staticClass:"line-number"},[t._v("53")]),s("br"),s("span",{staticClass:"line-number"},[t._v("54")]),s("br"),s("span",{staticClass:"line-number"},[t._v("55")]),s("br")])]),s("div",{staticClass:"language-python line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("construct_model")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" csv_path"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 读取数据")]),t._v("\n    df "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ProphetUtils"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("gen_df_from_csv_file"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("csv_path"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    train_df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" test_df "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ProphetUtils"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("df_split"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    train_ts "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ProphetUtils"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("df_to_ts"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train_df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" index_column"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'ds'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" val_column"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'y'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 数据平稳性检查")]),t._v("\n    debout "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ProphetUtils"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("gen_test_stationarity"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train_ts"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    logger"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("info"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("debout"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" debout"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'p-value'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.001")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("log_flag "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("log_flag"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        train_ts "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ProphetUtils"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ts_log"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train_ts"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 移动平均和差分")]),t._v("\n    train_ts_log_diff "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ProphetUtils"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("diff_ts"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train_ts"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" d"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("diff_list"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 训练集转换为df")]),t._v("\n    train_log_diff_df "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ProphetUtils"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ts_to_df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train_ts_log_diff"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" index_column"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'ds'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" val_column"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'y'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 调用模型进行训练")]),t._v("\n    self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("gen_holidays"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    forecast "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("build_model_predict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train_log_diff_df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                        period"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\n                                        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    forecast_ts "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ProphetUtils"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("df_to_ts"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("forecast"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" val_column"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'yhat'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    forecast_diff_reduction "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ProphetUtils"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict_diff_recover"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("forecast_ts"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" d"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("diff_list"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("log_flag"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        forecast_diff_reduction "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ProphetUtils"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ts_log_reduction"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("forecast_diff_reduction"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    forecast_df "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ProphetUtils"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ts_to_df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("forecast_diff_reduction"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" val_column"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'yhat'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    df_final "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model_accuracy_calculate"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("forecast"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("forecast_df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" df_test"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("test_df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    logger"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("info"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'test_data样本准确率：min=%.4f, max=%.4f, mean=%.4f, median=%.4f'")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("min")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df_final"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'rate'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("max")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df_final"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'rate'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                 np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mean"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df_final"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'rate'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("median"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df_final"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'rate'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" df_final\n")])]),t._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[t._v("1")]),s("br"),s("span",{staticClass:"line-number"},[t._v("2")]),s("br"),s("span",{staticClass:"line-number"},[t._v("3")]),s("br"),s("span",{staticClass:"line-number"},[t._v("4")]),s("br"),s("span",{staticClass:"line-number"},[t._v("5")]),s("br"),s("span",{staticClass:"line-number"},[t._v("6")]),s("br"),s("span",{staticClass:"line-number"},[t._v("7")]),s("br"),s("span",{staticClass:"line-number"},[t._v("8")]),s("br"),s("span",{staticClass:"line-number"},[t._v("9")]),s("br"),s("span",{staticClass:"line-number"},[t._v("10")]),s("br"),s("span",{staticClass:"line-number"},[t._v("11")]),s("br"),s("span",{staticClass:"line-number"},[t._v("12")]),s("br"),s("span",{staticClass:"line-number"},[t._v("13")]),s("br"),s("span",{staticClass:"line-number"},[t._v("14")]),s("br"),s("span",{staticClass:"line-number"},[t._v("15")]),s("br"),s("span",{staticClass:"line-number"},[t._v("16")]),s("br"),s("span",{staticClass:"line-number"},[t._v("17")]),s("br"),s("span",{staticClass:"line-number"},[t._v("18")]),s("br"),s("span",{staticClass:"line-number"},[t._v("19")]),s("br"),s("span",{staticClass:"line-number"},[t._v("20")]),s("br"),s("span",{staticClass:"line-number"},[t._v("21")]),s("br"),s("span",{staticClass:"line-number"},[t._v("22")]),s("br"),s("span",{staticClass:"line-number"},[t._v("23")]),s("br"),s("span",{staticClass:"line-number"},[t._v("24")]),s("br"),s("span",{staticClass:"line-number"},[t._v("25")]),s("br"),s("span",{staticClass:"line-number"},[t._v("26")]),s("br"),s("span",{staticClass:"line-number"},[t._v("27")]),s("br"),s("span",{staticClass:"line-number"},[t._v("28")]),s("br"),s("span",{staticClass:"line-number"},[t._v("29")]),s("br"),s("span",{staticClass:"line-number"},[t._v("30")]),s("br"),s("span",{staticClass:"line-number"},[t._v("31")]),s("br"),s("span",{staticClass:"line-number"},[t._v("32")]),s("br"),s("span",{staticClass:"line-number"},[t._v("33")]),s("br")])]),s("p",[t._v("预测表现结果如下")]),t._v(" "),s("p",[s("img",{attrs:{src:"/donot-eat-fish/img/data_analysis/time_series//predict_result.png",alt:"image-20191205152416970"}})]),t._v(" "),s("p",[t._v("样本数据为2018-01-01～2019-09-20这段时间的数据，使用fbprophet建模，经过一些优化和因子改变后，预测2019-09-21～2019-10-20的值，由于图片原因只展示了最后七天的数据，其中中位数和均值根据预测30日的结果计算得到。")]),t._v(" "),s("h2",{attrs:{id:"_3-后续的优化"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-后续的优化"}},[t._v("#")]),t._v(" 3.后续的优化")]),t._v(" "),s("ul",[s("li",[t._v("在对法定节假日进行预测时，待数据丰富后对法定节假日进行分离，分开建模预测，以提高其预测准确率。")]),t._v(" "),s("li",[t._v("经过产品及技术方案的评审后，将短期预测与监控工具进行结合，对每日的实际值和预测值进行对比监控。")]),t._v(" "),s("li",[t._v("与BI看板相结合，对于预测值以虚线展示，提高数据利用的宽度和深度。")])]),t._v(" "),s("h2",{attrs:{id:"附录-随机序列与时间序列"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#附录-随机序列与时间序列"}},[t._v("#")]),t._v(" 附录-随机序列与时间序列")]),t._v(" "),s("h3",{attrs:{id:"基本概念"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#基本概念"}},[t._v("#")]),t._v(" 基本概念")]),t._v(" "),s("p",[t._v("首先介绍一下期望的性质")]),t._v(" "),s("p",[t._v("​\t\t 1、设Y_t 是随机变量，c为常数，则Ε(cY_t )=cΕ(Y_t )")]),t._v(" "),s("p",[t._v("​\t\t 2、设Y_t和Y_s是两个随机变量，则Ε(Y_t+Y_s )=Ε(Y_t )+Ε(Y_s )")]),t._v(" "),s("p",[t._v("​         3、设Y_t和Y_s是两个相互独立的随机变量，则Ε(Y_t Y_s )=Ε(Y_t )Ε(Y_s )")]),t._v(" "),s("p",[t._v("​\t然后我们给定随机序列：{Y_t}={Y_t:t=0,±1,±2,±3,…}，那么它的均值函数就是它的期望，记作μ_t，方差就是随机变量Yt与它的均值函数的差值作平方，之后求期望，这里的Var就是variance。")]),t._v(" "),s("p",[s("img",{attrs:{src:"/donot-eat-fish/img/data_analysis/time_series//gongshi01.png",alt:"image-20191205152416970"}})]),t._v(" "),s("p",[t._v("​\t结合期望的性质可以得出一个重要的结论：")]),t._v(" "),s("p",[s("img",{attrs:{src:"/donot-eat-fish/img/data_analysis/time_series//gongshi02.png",alt:"image-20191205152416970"}})]),t._v(" "),s("p",[t._v("​\t和的协方差就可以展开为协方差的和，当"),s("img",{attrs:{src:"/donot-eat-fish/img/data_analysis/time_series//gongshi03.png",alt:"image-20191205152416970"}}),t._v(" 相同的时候，协方差就可以变成方差，所以就给出了一个特别的情况：")]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("也就是说，随机变量的和的方差可以展开称随机变量的方差求和再加上任何两个随机变量的协方差求和，这两个结论在计算不同的时间序列的时候会用到，可以根据期望的性质推导得到。\n")])])]),s("h3",{attrs:{id:"平稳性"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#平稳性"}},[t._v("#")]),t._v(" 平稳性")]),t._v(" "),s("p",[s("img",{attrs:{src:"/donot-eat-fish/img/data_analysis/time_series//gongshi04.png",alt:"image-20191205152416970"}})]),t._v(" "),s("p",[t._v("严平稳性只是要求了联合分布相同，但是并不代表随机变量Yt一定存在一阶炬和二阶矩函数。")]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("当n=1，其实代表Y_(t_1 )和Y_(t_1-k)的联合分布相同，Y是具有相同边际分布，如果此时Y存在着一阶矩和二阶矩，那么对于任何的t和k，Y的均值和方差函数恒定为常数。\n\n当n=2时，其实Y_(t_1 ),Y_(t_2 )与Y_(t_1-k),Y_(t_2-k)的联合分布相同，其实就是二元分布相同，如果存在二阶矩，Y的协方差的只依赖于时间间隔\n")])])]),s("p",[t._v("​\t如果该随机过程的均值、方差是与时间t无关的常数，协方差是只与时间间隔k有关，与时间t无关的常数，则称该随机过程而生成的时间序列是"),s("strong",[t._v("弱平稳")]),t._v("的。")]),t._v(" "),s("p",[t._v("​\t常见的如白噪声的过程是平稳的，因为它的均值是0，方差为常数s^2，所有时间间隔的协方差均为零。但随机游走时非平稳的，尽管它的均值为常数，但方差是与t相关的非常数，不过，若令DXt=Xt-Xt-1，则随机游走过程的一阶差分（first difference）是平稳的。")]),t._v(" "),s("p",[t._v("​\t样本时间序列展现了随机变量的历史和现状，因此所谓随机变量基本性态的维持不变也就是要求样本数据时间序列的本质特征仍能延续到未来。我们用样本时间序列的均值、方差、协（自）方差来刻画该样本时间序列的本质特征。于是，我们称这些统计量的取值在未来仍能保持不变的样本时间序列具有平稳性。可见，一个平稳的时间序列指的是：遥想未来所能获得的样本时间序列，我们能断定其均值、方差、协方差必定与眼下已获得的样本时间序列等同。相反，如果样本时间序列的本质特征只存在于所发生的当期，并不会延续到未来，亦即样本时间序列的均值、方差、协方差非常数，则这样一个过于独特的时间序列不足以昭示未来，我们便称这样的样本时间序列是非平稳的。形象地理解，平稳性就是要求经由样本时间序列所得到的拟合曲线在未来的一段期间内仍能顺着现有的形态“惯性”地延续下去；如果数据非平稳，则说明样本拟合曲线的形态不具有“惯性”延续的特点，也就是基于未来将要获得的样本时间序列所拟合出来的曲线将迥异于当前的样本拟合曲线。可见，时间序列平稳是经典回归分析赖以实施的基本假设；只有基于平稳时间序列的预测才是有效的。如果数据非平稳，则作为大样本下统计推断基础的“一致性”要求便被破坏，基于非平稳时间序列的预测也就失效。")]),t._v(" "),s("h2",{attrs:{id:"参考资料"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#参考资料"}},[t._v("#")]),t._v(" 参考资料")]),t._v(" "),s("ol",[s("li",[s("a",{attrs:{href:"https://facebook.github.io/prophet/docs/quick_start.html#python-api",target:"_blank",rel:"noopener noreferrer"}},[t._v("Prophet官方文档"),s("OutboundLink")],1)]),t._v(" "),s("li",[s("a",{attrs:{href:"https://zh.wikipedia.org/zh-hans/%E6%99%82%E9%96%93%E5%BA%8F%E5%88%97",target:"_blank",rel:"noopener noreferrer"}},[t._v("维基百科-时间序列"),s("OutboundLink")],1)]),t._v(" "),s("li",[s("a",{attrs:{href:"https://www.ancii.com/axy3epmjp/",target:"_blank",rel:"noopener noreferrer"}},[t._v("时间序列预测"),s("OutboundLink")],1)])])],1)}),[],!1,null,null,null);s.default=r.exports}}]);