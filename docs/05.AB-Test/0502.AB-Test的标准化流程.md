---
title: AB-Test的标准化流程
date: 2023-03-14 11:28:40
permalink: /ab_test/8663f3/
categories:
  - A/B-Test
tags:
  - 
author: 
  name: 不爱吃鱼的bobo
---

## 一、确定一个好的目标和假设

### 1. A/B-Test能够解决什么业务问题

| 产品迭代                               | 算法优化                                           | 市场营销                 |
| -------------------------------------- | -------------------------------------------------- | ------------------------ |
| 如何改变用户的交互界面来提升用户的体验 | 如何通过提高算法推荐的准确度来提升用户的粘性       | 如何确定最优营销内容     |
| 如何优化新用户注册流程来提高转化率     | 如何通过提高搜索算法排名的准确度来提高用户的点击率 | 如何确定最优的营销时间   |
| 如何确定产品优惠券的最优价值           | 如何通过提高广告显示算法的精确度来提高广告的点击率 | 如何确定最精准的受众群体 |
| 如何增加产品功能来提升用户留存         | 如何通过距离算法的优化来计算乘客的最小路费         | 如何衡量市场营销的效果   |

- 所有的业务问题都有一个目标，比如提升用户粘性是业务问题的目标，同时我们也把这个目标称作**结果**。
- 有时候业务难题会有明确的努力方向，比如优化用户流程提升转化率，优化流程就是明确的努力方向。
- 有时有业务难题没有明确的努力方向，就需要我们仔细分析去发现原因，比如确定最优的有效内容。

### 2. 如何去确定目标和假设

1. 分析问题，确定想要达到的结果。
2. 提出业务问题的大体解决方案。
3. 从大体解决方案中提取出具体的假设。

一个好的假设应该是什么样子的

|        | 好的假设                                                     | 不好的假设                 |
| ------ | ------------------------------------------------------------ | -------------------------- |
| 来源   | 用户调研、数据挖掘、观察总结、数据分析等                     | 不基于事实和数据的主管猜测 |
| 因果   | 明确包含可能的原因和结果                                     | 可能的原因和结果不明确     |
| 可证伪 | 可被证伪                                                     | 模糊，很难证伪             |
| 可测量 | 定量的指标                                                   | 定性的结果                 |
| 例子   | 在某一个位置/操作结束后，增加某一个功能，可以带来某个指标的提升 | 我们的产品可以走向国际市场 |

- A/B 测试是因果推断，所以我们首先要确定原因和结果。
- 目标决定了结果（具体量化指标）， 而假设又决定了原因（具体动作），所以目标和假设对于 A/B 测试来说，是缺一不可。

## 二、如何选择一个合适的指标

### 1. 指标的分类

#### 1.1 评价指标（Evaluation Metrics）

​	一般是指能够驱动公司/组织**实现核心价值的指标**，又被称作驱动指标。

​	评价指标一般是**短期的，敏感的，有很强操作可行性**的。

​	例如：点击率、转化率、人均使用时长等

#### 1.2 护栏指标（Guardrail Metrics）

​	概括地说，护栏指标属于 A/B 测试中基本的合理性检验（Sanity Check），就像飞机起飞前的安全检查一样。它的作用就是作为辅助，来保障 A/B 测试的质量。

- 衡量A/B测试是否符合业务的长期目标，不会因为优化短期目标而打乱长期目标。
- 确保从统计上尽量减少出现各种偏差（Bias），得到尽可能值得信任的实验结果。

​	护栏指标作为辅助型指标，需要在评价指标选择好后进行确定。

### 2. 怎样选择评价指标

#### 2.1 评价指标的特征

- **可归因性**：我们选择的业务指标的变化（结果）必须要可以归因到实验中的变量（原因），需要做好控制标量。
- **可测量性**：能够很好的被量化和可测量，比如用户续订率就好于用户满意度。
- **敏感和稳定性**：如果实验中的变量变化了，评价指标要能敏感地做出相应的变化；但如果是其他因素变化了，评价指标要能保持相应的稳定性。业界通常采用 A/A 测试来测量稳定性，用回溯性分析来表征敏感性。
  - 和 A/B 测试类似，A/A 测试（A/A Test）也是把被测试对象分成实验组和对照组。但不同的是，A/A 测试中两组对象拥有的是完全相同的体验，如果 A/A 测试的结果发现两组的指标有显著不同，那么就说明要么分组分得不均匀，每组的数据分布差异较大；要么选取的指标波动范围太大，稳定性差。
  - 如果没有之前实验的数据，或者是因为某些原因（比如时间不够）没有办法跑新的实验，那我们也可以通过分析历史数据，进行**回溯性分析（Retrospective Analysis）**。也就是在分析之前不同的产品变化时，去看我们感兴趣的指标是否有相应的变化。

由此我可以获得下面两条经验：

1. **<font color='red'>用 A/B 测试来检测单次的变化时（比如单次推送 / 邮件）一般选用短期效果的指标，因为长期效果目标通常对单次变化并不敏感。</font>**

2. **<font color='red'>用 A/B 测试来检测连续的、永久的变化时（比如增加产品功能），可以选用长期效果的指标。</font>**

#### 2.2 选择合适的评价指标

1. **<font color='orange'>清楚业务和产品所处在的阶段，根据这个阶段的目标，来选择合适的评价指标。</font>**
2. **<font color='orange'>如果目标比较抽象，我们需要定性 + 定量来确定评价目标。</font>**
3. **<font color='orange'>如果有条件，通过公开或非公开的渠道，参考其他公司或组织的实验或者研究，根据自己的情况去借鉴他们的评价指标。</font>**

### 3. 综合多个指标，建立总体评价标准

​	结合多个指标，构建一个总体评价标准 （Overall Evaluation Criteria，简称 OEC）。

$$
OEC = \frac {(\Sigma_i Revevnue - S * Unsubscribe\_lifetime\_loss)} {n}
$$

- i，代表每一个用户。
- S，代表每组流失的用户数。
- Unsubscribe_lifetime_loss ，代表用户流失带来的预计的损失。
- n，代表每组的样本大小。

​	使用OEC的好处：

1. 综合各方面的指标，来把我整体的好坏，OEC中可能包含护栏指标。
2. OEC可以有效避免多重检验问题（Multiple Testing Problem）。
3. 不同指标的单位、大小可能不在一个尺度上，需要先要对各个指标进行归一化（Normalization）处理，使它们的取值都在一定的范围内，比如[0, 1]， 之后再进行结合，从而剔除指标单位 / 大小的影响。

### 4. 衡量评价指标的波动性

​	在统计学里面，指标的波动性通常用其平均值和平均值的标准差来表示，一个指标平均值的标准差越大，说明其波动性越大。这里面要注意，**变量平均值的标准差又叫做标准误差（Standard Error）**。

​	评价指标的正常波动范围，就是置信区间。

#### 4.1 根据统计公式来计算

​	**置信区间：**

$$
置信区间\:=\:样本均值（Sample\:Mean）\:±\:Z分数\:*\:标准误差 
$$

​	**标准误差：**


$$
Stand\:Error=\left\{
\begin{aligned}
\sqrt{\frac {p(1\:-\:p)}{n}}\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:{（概率类指标）}\\\\
\sqrt{\frac {s^2}{n}}\:=\:\sqrt{\frac{\Sigma^n_i{(x_i\:-\:\overline{x})^2}}{n(n\:-\:1)}} \:\:\:\:\:\:\:\:\:\:\:\:\:{（均值类指标）}
\end{aligned}
\right.
$$

​	其中，

- p 代表事件发生的概率
- s 代表样本的标准差
- n= 样本大小
- x<sub>i</sub>= 第 i 个用户的使用时长或者购买金额等
- $\overline{x}$= 用户的平均使用时长或者购买金额等。

#### 4.2 根据时间经验来确定

##### 4.2.1 A/A测试

​	我们可以跑多个不同样本大小的 A/A 测试，然后分别计算每个样本的指标大小，计算出来后，再把这些指标从小到大排列起来，并且去除最小 2.5% 和最大 2.5% 的值，剩下的就是 95% 的置信区间。

##### 4.2.2 Bootstraping算法

​	我们可以先跑一个样本很大的 A/A 测试，然后在这个大样本中进行随机可置换抽样（Random Sample with Replacement）， 抽取不同大小的样本来分别计算指标。然后采用和 A/A 测试一样的流程：把这些指标从小到大排列起来，并且去除最小 2.5% 和最大 2.5% 的值，得到 95% 的置信区间。

​	在实际应用中，Bootstrapping 会更流行些，因为只需要跑一次 A/A 测试，既节省时间也节省资源。

### 5. 护栏指标的选择

​	A/B 测试往往改变的是业务中的某一部分指标（评价指标），所以我们很容易只关注短期的改变，却失去了对业务的大局观（比如长期的盈利能力 / 用户体验）的掌控或者统计上合理性的检查。因此推荐每个 A/B 测试都要有相应的护栏指标。

​	护栏指标的选择可以从**业务品质和统计品质**两个方向进行选择。

<font color='#bf0031'>

| 业务品质 | 统计品质                      |
| -------- | ----------------------------- |
| 网络延迟 | 实验组/对照组样本量大小的比例 |
| 闪退率   | 实验组/对照组中特征的分布     |
| 人均指标 |                               |

</font>

#### 5.1 业务品质

##### 5.1.1 网络延迟

​	**网页加载时间、App 响应时间等，都是表征网络延迟的护栏指标**。增加产品功能可能会增加网页或 App 的响应时间，而且用户可以敏感地察觉出来。这个时候，就需要在 A/B 测试中加入表征网络延迟的护栏指标，确保在增加产品功能的同时，尽可能减少对用户体验的影响 （一般是通过优化底层代码）。

##### 5.1.2 闪退率

​	程序崩溃和闪退是非常影响用户的效率，很容易失去用户。所以，在测试应用程序的新功能和大改动时，尤其是针对一些大的改动，闪退率就是一个比较好的护栏指标。

##### 5.1.3 人均指标

​	人均指标可以从两个角度来考虑：

- 收入角度，比如人均花费、人均利润等。代表了产品的盈利能力。
- 用户参与度，比如人均使用时长、人均使用频率等。某种程度上代表用户的满意程度。

#### 5.2 统计品质

​	统计方面主要是尽可能多地消除偏差，使实验组和对照组尽可能相似，比如**检测两组样本量的比例，以及检测两组中特征的分布是否相似**。造成偏差的原因有很多，可能是随机分组的算法出现了 Bug，也可能是样本不够大，还有可能是触发实验条件的数据出现了延迟，不过更多的是具体实施中的工程问题造成的。

​	这些偏差都会影响我们获得准确的实验结果，而护栏指标就是我们发现这些偏差的利器。

##### 5.2.1 实验组/对照组样本量大小的比例

​	在设计 A/B 测试的时候，我们就会预先分配好实验组和对照组，通常是把样本等分。也就是说，实验组和对照组样本大小的比例，预期是 1:1=1。但有的时候，当实验结束后却发现两者的比例并不等于 1，甚至也没有很接近 1。这就说明这个实验在具体实施的过程中出现了问题，导致实验组和对照组出现了偏差。

##### 5.2.2 实验组/对照组中特征的分布

​	A/B 测试中一般采取随机分组，来保证两组实验对象是相似的，从而达到控制其他变量、只变化我们关心的唯一变量（即 A/B 测试中的原因）的目的。

​	比如说，如果以用户作为实验单位的话，那么，在试验结束后去分析两组数据时，两组中用户的年龄、性别、地点等基本信息的分布应该是大体一致的，这样才能消除偏差。否则，实验本身就是有问题的，得出的结果也不是可信赖的。

## 三、选取合适的实验单位

​	实验单位包括<font color='orange'>**用户层面、访问层面、页面层面**</font>三个方向，在每个方向上都有着不同的实验单位，**实验单位的准确度越高，A/B 测试结果的准确度才会越高。**	

| 用户层面                                                     | 访问层面                                                     | 页面层面                                                   |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ---------------------------------------------------------- |
| 用户ID，用户登陆、注册的ID，手机号、邮箱、user_id            | 把用户的每次访问作为一个最小单位，给定一个访问ID或会话ID     | 页面层面指的是把每一个新的页面浏览（Pageview）作为最小单位 |
| 匿名ID，用户浏览时的cookie                                   | 考虑到用户访问的复杂性，通常情况下，如果用户在某个网站、App 连续 30 分钟之内没有任何动作，系统就认定这次访问已经结束了。 |                                                            |
| 设备ID，设备出厂时绑定，区分设备，无法区分多个用户共享一个设备 |                                                              |                                                            |
| IP地址                                                       |                                                              |                                                            |
| 准确度由高到低：用户 ID > 匿名 ID（Cookies）/ 设备 ID > IP 地址。 | 一个用户经常访问，如果以访问层作为实验单位，可能出现一个用户同时出现在实验组和对照组 |                                                            |

​	通过上面各层实验单位的区分，我可以做如下对比：

1. **访问层面和页面层面的单位，比较适合变化不易被用户察觉的 A/B 测试，比如测试算法的改进、不同广告的效果等等；如果变化是容易被用户察觉的，那么建议你选择用户层面的单位。**
2. **从用户层面到访问层面再到页面层面，实验单位颗粒度越来越细，相应地可以从中获得更多的样本量。原因也很简单，一个用户可以有多个访问，而一个访问又可以包含多个页面浏览。**

### 1. 选取实验单位的原则

1. **保证用户体验的连贯性。**
2. **实验单位应与评价指标的单位保持一致。**
3. **样本数量要尽可能多。**

#### 1.1 保证用户体验的连贯性

​	如果A/B测试中对照组和实验组的变化，用户是能够感知到的，那么这个时候就需要选择用户层面的实验单位，避免用户产生困扰，这个功能时而能用时而不能用的感觉。

#### 1.2 实验单位与评价指标的单位保持一致

​	A/B 测试的一个前提是**实验单位相互独立且分布相同的（Independent and identically distributed），简称 IID**。如果两个单位不一致，就会违反相互独立这一前提，破坏了 A/B 测试的理论基础，从而导致实验结果不准确。

#### 1.3 样本数量要尽可能多

​	在 A/B 测试中，样本数量越多，实验结果就越准确。但增加样本量的方法有很多，我们绝对不能因为要获得更多的样本量，就选择颗粒度更细的实验单位，而不考虑前面两个原则。

​	所以我们选取实验单位的第三个原则就是：在保证用户体验连贯性、实验单位和评价指标的单位一致的前提下，可以尽可能地选择颗粒度更细的实验单位来增加样本量。

### 2. 实验单位选择决策图



<center><img src="/donot-eat-fish/img/ab_test/实验单位选取决策图.png" /></center>

## 四、选取实验样本量
## 五、分析测试结果

