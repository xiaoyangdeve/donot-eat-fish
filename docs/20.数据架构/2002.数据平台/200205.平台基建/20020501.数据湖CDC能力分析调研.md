---
title: 数据湖CDC能力分析调研
date: 2023-09-14 16:30:58
permalink: /pages/e4a2cf/
author: 
  name: 何安平
tags: 
  - 技术系列
  - 数据湖
categories: 
  - 数据架构
  - 数据平台
  - 平台基建
---
> 技术本身无好坏之分，依赖业务场景才能做出更好的选型<br/>
> 通过本文可以了解到目前数据湖现状和CDC入湖选型知识

# 1.什么是数据湖？
底层依赖于HDFS，S3等分布式存储系统，通过Delta Lake、Iceberg 和 Hudi作为元数据管理系统，上层通过Spark、Flink、Presto等进行数据写入和分析的完整的超大规模数据服务系统。
# 2.眼花缭乱的数据湖技术
## 2.1 Delta-Databricks(Github 6.7k star)
Delta Lake作为 Databricks 开源的项目，更侧重于在 Spark 层面上解决 Parquet 等存储格式的固有问题，并带来更多的能力提升。Delta Lake 高度依赖于 Spark 生态圈，与其他引擎的适配尚需时日。<br />
![img.png](../.vuepress/public/img/data_lake_cdc/img.png)
![img_1.png](../.vuepress/public/img/data_lake_cdc/img_1.png)
![img_25.png](../.vuepress/public/img/data_lake_cdc/img_25.png)
- ACID Transactions：支持关系型数据库的ACID特性
- Scalable Metadata Handling：支持对元数据的分布式处理
- Time Travel ：数据版本
- Open Format：开放的数据存储格式
- Unified Batch and Streaming Source and Sink：批流统一
- Schema Enforcement：支持schema检查
- Schema Evolution：支持schema动态变化
- Audit History：日志审计
- Merge、Updates and Deletes：支持多样的数据库操作
- 100% Compatible with Apache Spark API：完全兼容Spark API
## 2.2 Iceberg-Netflix(Github 5.3k star)
Iceberg设计初衷更倾向于定义一个标准、开放且通用的数据组织格式，同时屏蔽底层数据存储格式上的差异，向上提供统一的操作 API，使得不同的引擎可以通过其提供的 API 接入。Iceberg 在其格式定义和核心能力上最为完善，但是上游引擎的适配上明显不足，比如 Spark、Hive、Flink 等不同引擎的适配， 而对于 Hive、Flink 的支持尚在开发中（**目前趋于完善**）。Iceberg 在数据组织方式上充分考虑了对象存储的特性，避免耗时的 listing 和 rename 操作。<br />
![img_2.png](../.vuepress/public/img/data_lake_cdc/img_2.png)<br />
![](https://camo.githubusercontent.com/d120d4367f4fcaea0086ec2533ecad35c4ce2fadc313071ee2c26ff319833168/68747470733a2f2f696365626572672e6170616368652e6f72672f646f63732f6c61746573742f696d672f496365626572672d6c6f676f2e706e67#from=url&id=IbgAX&originHeight=218&originWidth=800&originalType=binary&ratio=1&rotation=0&showTitle=false&status=done&style=none&title=)
## 2.3 Hudi-Uber(Github 5k star)
Hudi 的设计初衷是为了解决流式数据的快速落地，并能够通过 upsert 语义进行延迟数据修正。Hudi 基于 Spark 打造了完整的流式数据落地方案，但是其核心抽象较弱，与 Spark 耦合较紧，设计高度复杂。Hudi on Flink由T3出行解耦Spark实现，现由阿里云玉兆老师主力研发。<br />![img3.png](../.vuepress/public/img/data_lake_cdc/img3.png)<br/>
![img_24.png](../.vuepress/public/img/data_lake_cdc/img_24.png)
### 2.3.1 Hudi File Layout
![img_3.png](../.vuepress/public/img/data_lake_cdc/img_3.png)
<br />File group -> Bucket

- File slice -> Version（多次写入，形成不同的flie slice）
   - Parquet（base file ，无重复）（mor）
   - Log（有重复数据）（mor）
### 2.3.2 Hudi Index
![img_4.png](../.vuepress/public/img/data_lake_cdc/img_4.png)<br />根据flink state判断<br />若是update数据则直接写到历史数据所在的file group（bucket） -> file slice(最新的)<br />第一次写全新的数据根据，优先写入未写满的file group（bucket）
### 2.3.3 Async Compaction
![img_5.png](../.vuepress/public/img/data_lake_cdc/img_5.png)
### 2.3.4 Table Types
| **Trade-off** | **CopyOnWrite** | **MergeOnRead** |
| --- | --- | --- |
| Data Latency | Higher | Lower |
| Query Latency | Lower | Higher |
| Update cost (I/O) | Higher (rewrite entire parquet) | Lower (append to delta log) |
| Write Amplification | Higher | Lower (depending on compaction strategy) |

## 2.4 Paimon-Flink Community(Github 1.7k star)
前面的三马车不够猛？Paimon又是什么鬼？<br />![img_6.png](../.vuepress/public/img/data_lake_cdc/img_6.png)<br />Apache Paimon 就是一个专门为 CDC 处理、流计算而生的数据湖，希望为用户带来舒服、自动湖上流处理体验。Paimon 的前身是 Flink 社区开发的 Flink Table Store。其架构参考了 Iceberg 的一些设计理念。
![img_23.png](../.vuepress/public/img/data_lake_cdc/img_23.png)
## 2.5 总结
![img_7.png](../.vuepress/public/img/data_lake_cdc/img_7.png)<br />如果用一个比喻来说明 Delta、Iceberg、Hudi、Hive-ACID 四者差异的话，可以把四个项目比做建房子。

1. Delta 的房子底座相对结实，功能楼层也建得相对高，但这个房子其实可以说是 Databricks 的。目前今天 Delta 依然是 Databricks 的，也对接了其它引擎，但成熟度不高，开源版本距离 Databricks 商业版也有一定差距。
2. Iceberg 的建筑基础非常扎实，扩展到新的计算引擎或者文件系统都非常的方便，但是现在功能楼层相对低一点。目前今天 Iceberg 建筑扎实，北美数仓 SAAS 强者众多，各个批数仓对接 Iceberg，在北美风生水起。
3. Hudi 的情况有所不同，它的建筑基础设计不如 iceberg 结实，举个例子，如果要接入 Flink 作为 Sink 的话，需要把整个房子从底向上翻一遍，同时还要考虑不影响其他功能；但是 Hudi 的楼层是比较高的，功能是比较完善的。目前Hudi 功能多，各种功能都有，目前在国内高度流行，**Upsert 写入在中国打开了局面**。
4. Hive-ACID 的房子，看起来是一栋豪宅，绝大部分功能都有，但是细看这个豪宅的墙面是其实是有一些问题的。
5. Paimon直接把Iceberg的家借鉴过来，然后结合LSM Tree打造Streaming WareHouse，真正的全增量一体，流批一体数据湖。
# 3.数据CDC入湖该怎么选？
> CDC是什么？

binlog是记录所有数据库表结构变更（例如CREATE、ALTER TABLE…）以及表数据修改（INSERT、UPDATE、DELETE…）的二进制日志。

1. Binlog CDC：实时捕获数据变更产生的变更日志
2. JSON串语义：INSERT：op为‘c’，before为null，after为数据UPDATE：op为‘u’，before修改之前的数据，after为修改之后的数据DELETE：op‘d’，before为数据，after为null
3. CDC数据必须保持顺序，乱序会导致JSON串合并错误最终导致数据不一致

![img_8.png](../.vuepress/public/img/data_lake_cdc/img_8.png)
## 3.1 Iceberg CDC-半自动步枪
扎实的房子对我们来说更具有诱惑力，我们坚信能基于 Iceberg 打造出属于 Streaming 的 Lakehouse。<br />![img_9.png](../.vuepress/public/img/data_lake_cdc/img_9.png)<br />现阶段性产出是 Flink + Iceberg 进入生产可用，Flink 入湖成为主流应用，CDC 入湖基本可用。Iceberg V2表有MOR的实现，写入仅两步：1）根据`update`或者`delete`的条件，找到文件中匹配的记录，记录他们在文件中的offset，然后持久化到一个bin文件中；2）将更新后的数据，写到普通的一个新文件中。详情见[Apache Flink 学习网](https://flink-learning.org.cn/article/detail/b03b963655351da5d7442bdeb1ca6c57) | [Iceberg Spec](https://iceberg.apache.org/spec/)

### 3.1.1 新的挑战
目前Iceberg CDC 只是基本可用，离我们的想象还差的较远，大规模更新与近实时延时都有距离，更别说流读的增强了，主要的原因有如下：

1.  Iceberg 社区基本盘还是在离线处理，它在国外的应用场景主要是离线取代 Hive，它也有强力的竞争对手 Delta，很难调整架构去适配 CDC 流更新。 
2.  Iceberg 扩展性强，对其它计算引擎也暴露的比较多的优化空间，但是这也导致后续的发展难以迅速，涉及到众多已经对接好的引擎。 

这并没有什么错，后面也证明了 Iceberg 主打离线数据湖和扩展性是有很大的优势，得到了众多国外厂商的支持。
## 3.2 Hudi CDC-全自动步枪
Hudi率先通过COW、MOR两种抽象模式，将批流数据统一存储计算。如图所示，对于COW模式，在时间轴的某个时间点假设有三组数据，其中包含数据文件为flie1、flie2和flie3<br />![img_10.png](../.vuepress/public/img/data_lake_cdc/img_10.png)<br />当file1和file2经过一次批量更新后分别从version1转化为version2，其中数据存在新增、更新与删除操作，file3则是将全部数据删除，同时创建了File group4用于存放file4。每次对COW中的数据操作时，Hudi会通过合并（Merge）新数据和旧数据机制产生新的文件，因此COW存在写放大的问题（Write Amplification），但COW不存在压缩（Compact）操作，新产生的数据文件即为最终文件（Parquet、Orc），简化了数据优化就绪过程。
### 3.2.1 MOR
MOR用于处理流数据，如图所示，数据合并的策略从数据写入端转移至读取端，数据写入期间不会合并或者创建新的数据文件版本。<br />![img_11.png](../.vuepress/public/img/data_lake_cdc/img_11.png)

1. MOR首先为分发至文件组1（File group 1）的每条数据构建索引以区分数据是新增还是更新，然后根据索引会尽量将数据追加到未达到指定数据块阈值的预写日志（Delta log） 文件中，利用该操作可减少部分小文件产生。每次将数据追加到File group1后会产生一个数据合并计划（Compaction Plan），累计定量的计划则会执行数据合并（Compact）操作，将Data file version1的基础文件（Base file）和新追加的Delta log 文件进行合并，产生新的版本数据文件，即Data file version2。MOR模式有利于流数据的摄入，新增的数据通过追加顺序写入Delta log文件，数据写入效率能够得到保证，
2. 然而，由于Delta log是基于avro的行文件，不利于分布式计算引擎的读取，故利用LSM Tree中Compact操作的思想，将多个Delta log文件和列基础文件（基础文件也是由Delta log文件转化）进行合并，最终产生读取优化的列存储文件（Parquet、Orc）。
3. 目前MOR的数据写方案中，需要区分每条数据是新增还是更新，即数据Tagging，同时由于流计算引擎逐条计算的特性，会频繁读取索引文件，产生大量的I/O操作，而且随着文件数量增加，构建索引的耗时也会随之增高，同时为了将行文件转化为读取优化的列文件，需将大量的Delta log文件进行Compact操作，在高速率的数据流量场景下，会严重影响流计算集群的稳定性和数据处理时延，甚至导致集群崩溃。

**写流程（upserts）详解**

- 先对 records 按照 record key 去重（可选）
- 首先对这批数据创建索引 (HoodieKey => HoodieRecordLocation)；通过索引区分哪些 records 是 update，哪些 records 是 insert（key 第一次写入）
- 如果是 insert 消息，如果 log file 不可建索引（默认），会尝试 merge 分区内最小的 base file （不包含 log file 的 FileSlice），生成新的 FileSlice；如果没有 base file 就新写一个 FileGroup + FileSlice + base file；如果  log file 可建索引，尝试 append 小的 log file，如果没有就新写一个  FileGroup + FileSlice + base file
- 如果是 update 消息，写对应的 file group + file slice，直接 append 最新的 log file（如果碰巧是当前最小的小文件，会 merge base file，生成新的 file slice）
- log file 大小达到阈值会 roll over 一个新的

**Flink State Index**<br />Hudi 默认使用 Flink State 来保存 Key 到 FileGroup 的 Index，这是一套和Hudi-Spark 完全不同的玩法：

1.  好处是全自动，想 Scale Up 只用调整并发就行了。 
2.  坏处是性能差，直接让湖存储变成了实时点查，超过 5 亿条数据性能更是急剧下降。 
3.  存储成本也高，RocksDB State 保存所有索引。 
4.  数据非常容易不一致，甚至再也不能有别的引擎来读写，因为一旦读写就破坏了 State 里面的 Index。 

针对 Flink State Index 诸多问题，字节跳动的工程师们在 Hudi 社区提出了 Bucket Index 的方案 Hudi Bucket Index 在字节跳动的设计与实践
### 3.2.5 新的挑战

1. 功能多，参数多，调优异常困难，非常考验研发经验
2. 系统设计复杂度甚高，兼容性较差，Bug多
3. 更新效率较低，容易反压，原始版本10w+ QPS/s只能通过离线异步做`Compaction`或者进行大的升配，否则直接干崩集群
4. Hudi 天然面向 Spark 批处理模式设计而诞生，不断在面向批处理的架构上进行细节改造，无法彻底适配流处理更新场景，在批处理架构上不断强行完善流处理更新能力，导致架构越来越复杂，可维护性越来越差。
5. Hudi 的稳定性随着近几个版本已经好很多了，这可以归功于来自中国的开发者和使用者们，他们解决 Hudi 各种各样的稳定性和正确性的漏洞，一步一步踩坑探索实时数据湖。
6. 通过订阅社区开发邮件和社区Roadmap可以看出，不怎么带Flink一起玩，新Feature基本都是为Spark做的设计，兼容Flink得把房子翻修
7. Hudi 是个好系统，但它不是为实时CDC数据湖而生的。
> [Hudi 的一些设计](https://www.yuque.com/yuzhao-my9fz/kb/eitru7?view=doc_embed)
> [HUDI FLINK 答疑解惑](https://www.yuque.com/yuzhao-my9fz/kb/flqll8?view=doc_embed)

## 3.3 我们需要怎样的数据湖？

1. 一个湖存储有着类似 Iceberg 良好的建筑基础，功能满足湖存储的基本诉求。 
2.  全增量一体，一个具有很强 Upsert 能力的存储，需要 OLAP 系统 & 流计算 State & KV 系统 都使用的 LSM 结构。 
3. 一个 Streaming First，面向 Flink 有最好集成的存储，这座房子的地基应该直接考虑 Streaming & Flink 的场景，而不是在一个复杂的系统上修修补补，越走到后面，越吃力(Hudi)。 
4. 一个更多面向中国开发者以及使用者的社区，而且社区的主方向应该是长期投入 Streaming + Lake。 
## 3.4 Paimon CDC-全自动火炮
### 3.4.1 Paimon CDC现状
![img_12.png](../.vuepress/public/img/data_lake_cdc/img_12.png)<br />Paimon 集成的 Flink CDC 在开源社区提供了非常方便一键入湖，可以将 Flink CDC 数据同步到 Paimon 中，也可以通过**整库**同步作业把整个库成百上千的表通过一个作业同步到多个 Paimon 表中。

如上图可见，Paimon 在开源社区做的 CDC 入湖不只是有 CDC 入湖单表同步和整库同步，也有 Kafka 单表同步和整个同步。如果这些还不能满足，用户如果有自己的消息队列和自己的格式，也可以通过 `RichCdcRecord` 这种编程方式达到入湖的效果。

**基于Paimon的数据湖计算链路生态**<br />![img_13.png](../.vuepress/public/img/data_lake_cdc/img_13.png)
> [Paimon - Apache Doris](https://doris.apache.org/docs/dev/lakehouse/multi-catalog/paimon)
> [Paimon StarRocks](https://docs.starrocks.io/zh-cn/latest/data_source/catalog/paimon_catalog)
> [Paimon Trino](https://paimon.apache.org/docs/master/engines/trino/)

### 3.4.2 A little feature~
> Ecosystem

![img_14.png](../.vuepress/public/img/data_lake_cdc/img_14.png)
#### 3.4.2.1 Merge Engines
> 受益于LSM Tree引擎，Paimon和ClickHouse有着许多相似的抽象

##### Deduplicate模型
重复合并引擎是默认的合并引擎。Paimon 只保留最新记录，而会删除主键相同的其他记录。<br />具体来说，如果最新记录是一条 DELETE 记录，那么所有具有相同主键的记录都会被删除。
##### Partial Update模型

1. 通过指定 `'merge-engine' = 'partial-update'`，用户可以通过多次更新来更新记录的列，直到记录完整为止。这是通过使用同一主键下的最新数据逐个更新值字段实现的。但在此过程中，空值不会被覆盖(即新字段值为空，则不会更新原有值)。
2. 对于流式查询，`partial-update`合并引擎必须与`lookup`或`full-compaction``changelog producer`一起使用。
3. Partial 无法接收 `DELETE` 消息，因为无法定义该行为。您可以配置 `'partial-update.ignore-delete'='true'` 来忽略 DELETE 消息。
###### Sequence Group
序列字段可能无法解决`partial-update`表在多数据流更新时的混乱问题，因为在**多数据流**更新时，序列字段可能会被另一个数据流的最新数据覆盖。<br />因此，引入了部分更新表的序列组机制。可以解决：

1. 多流更新过程中出现混乱。每个流定义自己的序列组。
2. 真正的部分更新，而不仅仅是非null更新。（HBase和Hudi）
```sql
CREATE TABLE T (
    k INT,
    a INT,
    b INT,
    g_1 INT,
    c INT,
    d INT,
    g_2 INT,
    PRIMARY KEY (k) NOT ENFORCED
) WITH (
    'merge-engine'='partial-update',
    'fields.g_1.sequence-group'='a,b',
    'fields.g_2.sequence-group'='c,d'
);

INSERT INTO T VALUES (1, 1, 1, 1, 1, 1, 1);

-- g_2 is null, c, d should not be updated
INSERT INTO T VALUES (1, 2, 2, 2, 2, 2, CAST(NULL AS INT));

SELECT * FROM T; -- output 1, 2, 2, 2, 1, 1, 1

-- g_1 is smaller, a, b should not be updated
INSERT INTO T VALUES (1, 3, 3, 1, 3, 3, 3);

SELECT * FROM T; -- output 1, 2, 2, 2, 3, 3, 3
```
##### Aggregation模型
> Always set table.exec.sink.upsert-materialize to NONE in Flink SQL TableConfig.

```sql
CREATE TABLE MyTable (
  product_id BIGINT,
  price DOUBLE,
  sales BIGINT,
  PRIMARY KEY (product_id) NOT ENFORCED
) WITH (
  'merge-engine' = 'aggregation',
  'fields.price.aggregate-function' = 'max',
  'fields.sales.aggregate-function' = 'sum'
);
```
当前支持的聚合函数和数据类型包括：

1. `sum`: supports DECIMAL, TINYINT, SMALLINT, INTEGER, BIGINT, FLOAT and DOUBLE
2. `min/max`: support DECIMAL, TINYINT, SMALLINT, INTEGER, BIGINT, FLOAT, DOUBLE, DATE, TIME, TIMESTAMP and TIMESTAMP_LTZ
3. `last_value / last_non_null_value`: support all data types
4. `listagg`: supports STRING data type
5. `bool_and / bool_or`: support BOOLEAN data type
> aggregation merge engine must be used together with **lookup** or **full-compaction** changelog producer.

#### 3.4.2.2 Changelog Producers
流查询将不断产生最新的更改。这些更改可以来自底层表文件，也可以来自像 Kafka 这样的外部日志系统。与外部日志系统相比，来自表文件的更改具有更低的成本但更高的延迟(取决于创建快照的频率)。<br />通过在创建表时指定 `changelog-producer` 表属性，用户可以选择从文件生成的`changelog`模式。
##### None
默认情况下，不会对表的写入器应用额外的更改日志生成器。Paimon Source只能看到跨快照的合并更改，比如删除了哪些键以及某些键的新值。<br />但是，这些合并的更改不能形成完整的更改日志，因为我们不能直接从它们读取键的旧值。合并更改要求使用者“记住”每个键的值，并在不看到旧键的情况下重写这些值。然而，一些消费者需要旧的值来确保正确性或效率。<br />考虑消费者计算一些分组键（可能不等于主键）的总和。如果消费者只看到一个新值 5，它就无法确定应将哪些值添加到求和结果中。如果旧值是 4，就应该在结果中加上 1。但如果旧值是 6，它又应该从结果中减去 1。对于这类消费者来说，旧值非常重要。<br />总之，没有一个更新日志生产者最适合数据库系统等消费者。Flink 还有一个内置的 `normalize operator`，可以在状态中持久化每个键的值。不难看出，这个operator的成本很高，应尽量避免使用。<br />![img_15.png](../.vuepress/public/img/data_lake_cdc/img_15.png)
##### Input
通过指定`'changelog-producer' = 'input'`，Paimon 的writers会依赖他们的输入作为完整更新日志的来源。所有输入记录都将保存在独立的更新日志文件中(`changelog file`)，并由 Paimon 源提供给消费者。<br />当 Paimon 写入器的输入是完整的更新日志（如来自数据库 CDC 或由 Flink 有状态计算生成）时，可以使用 input changelog producer。<br />![img_16.png](../.vuepress/public/img/data_lake_cdc/img_16.png)
##### Lookup
如果你的输入无法生成完整的更新日志，但又想摆脱代价高昂的规范化运算符，你可以考虑使用 "查找 "更新日志生成器。<br />通过指定 `'changelog-producer' = 'lookup'`，Paimon 会在提交数据之前通过 `lookup` 生成更新日志。<br />![img_17.png](../.vuepress/public/img/data_lake_cdc/img_17.png)

| Option | Default | Type | Description |
| --- | --- | --- | --- |
| ##### lookup.cache-file-retention
 | 1 h | Duration | The cached files retention time for lookup. After the file expires, if there is a need for access, it will be re-read from the DFS to build an index on the local disk. |
| ##### lookup.cache-max-disk-size
 | unlimited | MemorySize | Max disk size for lookup cache, you can use this option to limit the use of local disks. |
| ##### lookup.cache-max-memory-size
 | 256 mb | MemorySize | Max memory size for lookup cache. |

> `look up` changelog-producer 支持` changelog-producer.row-deduplicate` 以避免为同一记录生成 -U, +U changelog。

##### Full Compaction
如果认为`lookup`消耗的资源过多，可以考虑使用`full-compaction`changelog producer，可以将数据写入和更新日志生成解耦，更适合**延迟较高**的场景（例如 10 分钟）。<br />指定`'changelog-producer'='full-compaction'`后，Paimon 会比较完全压缩的结果，并将不同之处生成 changelog。`changelog`的延迟会受到**完全压缩频率**的影响。<br />通过指定 `'full-compaction.delta-commits'='5'` 表属性，完全压缩会在 delta 提交（检查点）后持续触发。该属性默认设置为 1，因此每个`checkpoint`都会进行完全压缩并生成变更日志。<br />![img_18.png](../.vuepress/public/img/data_lake_cdc/img_18.png)
> `full-compaction`更新日志生成器可以为任何类型的源生成完整的更新日志。不过，它的效率不如`input`更新日志生成器，而且生成更新日志的延迟可能会很高。
> `look up` changelog-producer 支持` changelog-producer.row-deduplicate` 以避免为同一记录生成 -U, +U changelog。

#### 3.4.2.3 Sequence Field
默认情况下，主键表根据输入顺序决定合并顺序（最后输入的记录将最后合并）。但是，在分布式计算中，有些情况下会导致数据混乱。此时，可以使用时间字段作为 sequence.field <br />更新或删除记录时，`sequence.field`必须变大，不能保持不变。对于 -U 和 +U的 `sequence.field` 必须不同。<br />如果提供的`sequence.field`无法满足精度要求，如粗略的秒或毫秒，可以将 `sequence.auto-padding` 设置为秒到微秒或毫秒到微秒，这样系统就会将序列号的精度提高到微秒。
```sql
CREATE TABLE MyTable (
    pk BIGINT PRIMARY KEY NOT ENFORCED,
    v1 DOUBLE,
    v2 BIGINT,
    dt TIMESTAMP
) WITH (
    'sequence.field' = 'dt'
);
```
无论输入顺序如何，`sequence.field` 值最大的记录将最后合并。
### 3.4.3 一些场景
#### 3.4.3.1 双流Join的场景
![img_19.png](../.vuepress/public/img/data_lake_cdc/img_19.png)

1. 第一种是 Flink 双流 join 的方式，需要维护两边比较大的 state，这也是成本比较高的原因之一。
2. 第二种是通过 Flink lookup join 的方式 lookup 到 Paimon 的数据，缺点是维表的更新不能更新到已经 join 的数据上。
3. 第三种是通过 Partial Update 的方式，即同组件的打宽的方式。推荐大家使用这种方式，它不仅具有高吞吐，还能带来近实时级别的延迟。
#### 3.4.3.2 消息队列代替
![img_20.png](../.vuepress/public/img/data_lake_cdc/img_20.png)

1. 既然 Paimon 面向的是实时，不免有些人就会拿 Paimon 和 Kafka 架构进行对比。Paimon 这边做了很多工作，比如它支持 Append-only 表，即你可以不定义主键，只定义 Bucket number。当定义 Bucket number 的时候，bucket 就类似 Kafka 的 partition 概念，做到了严格保序，跟 Kafka 的消息顺序是一模一样的，而且也支持 Watermark 且对齐。在写入的过程中，能够自动合并小文件，也支持 Consumer ID 消费。
2. Paimon 在提供消息队列能力的同时，也沉淀了所有的历史数据，而不是像 Kafka 一样只能保存最近几天的数据。
3. 所以通过业务图的方式可以看出，它的整体架构是想通过 Paimon 这种方式让用户在某些实时场景上替换 Kafka。Kafka 真正的能力是提供秒级延时，当业务不需要秒级延时的时候，可以考虑使用 Paimon 来替代消息队列。
#### 3.4.3.3 离线数仓功能
![img_21.png](../.vuepress/public/img/data_lake_cdc/img_21.png)

1. Paimon 是一个数据湖，数据湖最常见的应用是离线表。Paimon 也拥有这样的能力。
2. 在 Append 表定义的时候，把 Bucket 表定义为-1，那么 Paimon 就会认为这张表是一张离线表。Paimon 作为一张离线表可以替代原有的 Hive 数仓，比如 Paimon 支持批读批写，支持 INSERT OVERWRITE，也支持流读流写。而且 Paimon 可以自动合并小文件，也支持湖存储特性 ACID、Time Travel、Z-Order 排序加速查询和 Delete、Update 等等。
3. 综上所述，Paimon 基本上能做到大部分离线表的能力。
#### 3.4.3.4 视图管理
Paimon 的快照为查询历史数据提供了一种简便的方法。但在大多数情况下，作业会产生过多的快照，表会根据表的配置使旧快照过期。快照过期也会删除旧的数据文件，过期快照中的历史数据将无法再查询。<br />要解决这个问题，可以根据快照创建一个标签。标签将维护快照的清单和数据文件。典型的用法是每天创建标签，然后就可以维护每天的历史数据，以便批量读取。<br />![img_22.png](../.vuepress/public/img/data_lake_cdc/img_22.png)<br />众所周知，离线数仓有个非常重要的东西，就是它需要数据有一个不可变的视图，不然两次计算出的结果就不一样了。所以 Paimon 提供了一个非常重要的功能，即 Create Tag，它可以在 Paimon 中指定一些 Tag，让这些 Tag 永不删除，永远可读。
# 4.总结
不同的数据湖对于`CDC Ingestion`设计有较大的差异，Hudi是基于类LSM Tree的方式，先将数据通过行级log文件写入，利用`Compact`将文件转化为读友好的列存文件。Iceberg则是基于`Mixed position-delete and equality-delete`，将不同语义的数据写入多个文件，然后合并读取保证数据的正确性。而Paimon利用LSM Tree引擎先天的优势，完整的支持CDC的功能，同时在读写性能上也有着存储引擎级别的优势，开发方式上全增量一体和流批一体的设计能够减少数据开发的复杂度。在要求数据规模大、实时性能要求高、链路复杂度尽量低的数据入湖场景下，相信Paimon能够有所贡献。
