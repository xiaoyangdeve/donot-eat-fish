---
title: 数据开发常见问题笔记分享
date: 2024-07-01 10:46:23
permalink: /pages/d41efy/
author: 
  name: 苟元豪
tags: 
  - 技术系列
  - Spark
  - Hive
categories: 
  - 数据管理
  - 数据治理
---

## hive使用常见问题汇总：

#### 问题1:内存溢出(jave heap space)

​     会出现在两个地方，一个是mapreduce过程中，另外一个是Driver提交Job阶段
​     mapreduce过程中报错：
​     第一种报错：“java.lang.OutOfMemoryError: GC overhead limit exceeded“
​     第二种报错：“java.lang.OutOfMemoryError: Java heapspace”
​     第三种报错：“running beyondphysical memory limits.Current usage: 4.3 GB of 4.3 GBphysical memory used; 7.4 GB of 13.2 GB virtual memory used. Killing container”
​     

	 map阶段
	 解决：
	 发生OOM的几率很小，一般存在MapJoin才会出现这种OOM。
	 
	 1）如果存在MapJoin，设置参数set hive.auto.convert.join = false转成reduce端的Common Join。
	 其次一般就是切片太大了，尤其注意hdfs显示的大小是压缩后大小，如果切片设置的太大，解压后处理很容易撑爆内存。set mapred.max.split.size=256000000；
	 如果读取的源表是orc存储格式且数据量较大，调整参数：
	 set hive.exec.orc.default.buffer.size=10000;ORC缓冲区大小；
	 set hive.exec.orc.memory.pool=0.1。默认值:0.5 ORC文件写入器可以使用的堆的最大部分
	 2）shuffle阶段
	 这种一般是因为由于map的输出较大，但shuffle阶段选择的是拷贝map输出到内存导致。
	 降低单个shuffle能够消耗的内存占reduce所有内存的比例，使得shuffle阶段拷贝map输出时选择落盘
	 （set mapreduce.reduce.shuffle.memory.limit.percent=0.05），
	 3）reduce阶段
	 解决：单个reduce处理数据量过大，通过设置参数mapred.reduce.tasks 或mapreduce.job.reduces 修改reduce个数，
	 或者减少每个reduce处理的数据量，调整参数：hive.exec.reducers.bytes.per.reducer，默认300000000。
	 如果存在数据倾斜，单纯增加reduce个数没有用，需sql预处理。
	 
	 Driver提交Job阶段OOM
	 java.lang.OutOfMemoryError: GC overhead limit exceeded
	 at sun.nio.cs.UTF_8.newEncoder(UTF_8.java:53)
	 at java.beans.XMLEncoder.createString(XMLEncoder.java:572)
	 job产生的执行计划的条目太多，比如扫描的分区过多，上到4k-6k个分区的时候，并且是好几张表的分
	 区都很多时，究其原因，是因为序列化时，会将这些分区，即hdfs文件路径，封装为Path对象，这样，如果对象太多
	 了，而且Driver启动的时候设置的heap size太小，则会导致在Driver内序列化这些MapRedWork时，生成
	 的对象太多，导致频繁GC。
	 解决思路
	 ① 减少分区数量，将历史数据做成一张整合表，做成增量数据表，这样分区就很少了。
	 ② 调大Hive CLI Driver的heap size, 默认是256MB，调节成512MB或者更大。
	 具体做法是在bin/hive bin/hive-config里可以找到启动CLI的JVM OPTIONS。
	 这里我们设置
	 双管齐下， 即做成了整合，方便使用，又调节了Hive CLI Driver的heap size，保证线上的运行稳定。
	 
	 总结：
	 遇到这种问题：
	 
	 一是SQL的写法上，尽量少的扫描同一张表，并且尽量少的扫描分区。扫太多，一是job数多，
	 慢，二是耗费网络资源，慢。
	 二是参数调优和JVM的参数调优，尽量在每个阶段，选择合适的jvm max heap size来应对
	 OOM的问题。



#### 问题2：动态分区问题

​    1）动态分区, 要先设定partition参数
[Error 10096]: Dynamic partition strict mode requires at least one static partition column. To turn this off set hive.exec.dynamic.partition.mode=nonstrict；
​    动态分区, 要先设定partition参数。
​    解决：
​    #开启允许所有分区都是动态的，否则必须要有静态分区才能使用。
​    set hive.exec.dynamic.partition=true;
​    set hive.exec.dynamic.partition.mode=nonstrict;
​    2）分区太多
​    Failed with exception Number of dynamic partitions created is 1191, which is more than 1000. To solve this try to set hive.exec.max.dynamic.partitions to at least 1191
​    参数hive.exec.max.dynamic.partitions限制了所允许的最大分区个数，默认值是1000。
​    解决：调大参数hive.exec.max.dynamic.partitions。或者降低分区。
​    

#### 问题3：并发写数

​    Failed with exception copyFiles: error while moving files!!! Cannot move XXX to XXX
​    解决：一般都是由于并发写表数据导致的，规避多个任务或同一个任务的不同实例并发写同一个表的情况。

#### 问题4： Hive任务执行过程中失败，报“Caused by: java.io.FileNotFoundException: File does not exist: XXX”

​       原因：任务运行过程中依赖的表发生了变化，可能是表被删了、表重建了或者表数据重新生成之类的等等情况。
​	   2）报错：“Caused by: java.io.FileNotFoundException: File does not exist: hdfs://XXX/user/A/hive/warehouse/A.db/trec_season2_coupon_info_pc/000000_0”，
​	   则是因为任务运行过程中，依赖的表A.trec_season2_coupon_info_pc发生了变化。
​	   

#### 问题5：分区问题

​    1）对于Hive里的分区表，新增字段后，通过insert overwrite等方式重新生成历史分区的数据，发现新增字段仍旧是null值。
​    解决：Hive分区表新增字段后，需要对历史老分区进行重建才能查询新增字段，但是对于新增字段后新生成的分区是不受影响的，能正常查询数据。
​    2）Hive表分区字段类型使用string类型，不能用varchar，后者会扫全表。
​     hive的分区使用的是表外字段，分区字段是一个伪列，但是分区字段是可以做查询过滤。
​     分区字段不建议使用中文
​     一般不建议使用动态分区，因为动态分区会使用mapreduce来进行查询数据，如果分区数据过多，导致namenode和resourcemanager的性能瓶颈。所以建议在使用动态分区前尽可能预知分区数量。
​     分区属性的修改都可以修改元数据和hdfs数据内容。

#### 问题6：报错关键字Ambiguous column reference * in *

​       解决：去掉重复的字段
​       示例：select t.id from (select dummy,dummy from default.dual)t;
​       FAILED: SemanticException [Error 10007]: Ambiguous column reference dummy in t
​       去掉子查询里的一个dummy字段。
​       2）使用date等关键字作为字段时，得进行转义
​       示例：select * from orders where `date`='20240101';
​	   3）建表列名不允许包含.或:，报错如下：ParseException line 1:45 Failed to recognize predicate ')'. Failed rule: '[., :] can not be used in column name in create table statement.' in column specification
​       解决：检查建表语句，将字段名中包含的.或: 去掉。

#### 问题7：union all的子句里不支持orderByClause、clusterByClause、distributeByClause、sortByClause或limitClause

​      解决：改造hql，去掉union all子查询里的orderByClause、clusterByClause、distributeByClause、sortByClause和limitClause语句
​       示例：select t.id from (select dummy from default.dual limit 1 union all select dummy from default.dual limit 1)t;
​       去掉两个子查询里的limit 1;

#### 问题8： FAILED: SemanticException Number of partitions scanned (=xxxx) on table xxx exceeds limit (=xxx). This is controlled by hive.limit.query.max.table.partition.  

​        解决：检查报错的sql语句是否对涉及的分区表加上分区过滤，没有加上的请加上分区过滤。若已加上的，请缩小分区过滤区间。
​        加分区过滤有个特殊情况：分区表分区字段是日期类型时限制分区条件请使用to_unix_timestamp方法，如果使用unix_timestamp方法仍会扫全表分区。

#### 问题9： java.lang.IllegalArgumentException: Unknown primitive type VOID

​       解决：将create table t_orc as orc as select a,null as xx 中null转换为CAST(NULL AS bigint)

#### 问题10：Map阶段读取Hive json结构表报错，Row is not a valid JSON Object

​     例：
​     used by: org.apache.hadoop.hive.serde2.SerDeException: Row is not a valid JSON Object - JSONException: Missing value at 4246 [character 4247 line 1]
​      org.openx.data.jsonserde.JsonSerDe.onMalformedJson(JsonSerDe.java:397)
​      org.openx.data.jsonserde.JsonSerDe.deserialize(JsonSerDe.java:174)
​      org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.readRow(MapOperator.java:141)
​      org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.access$200(MapOperator.java:105)
​      org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:507)
​     . 9 more
​     决：查询的表数据文件里有记录是不正确的json格式，修改表属性：ALTER TABLE json_table SET SERDEPROPERTIES ( "ignore.malformed.json" = "true");来忽略错误记录避免任务报错，但处理结果会丢失这条记录。

#### 问题11：Hive创建文件数过多问题，报错信息类似如下：

​        total number of created files now is 100130, which exceeds 100000. Killing the job
​        解决：调大参数hive.exec.max.created.files  ，原因还是小文件过多，优先用小文件治理手段，一味调参下策







## Spark 常见故障解决☆☆

##### 1)、org.apache.spark.SparkException: Kryo serialization failed: Buffer overflowjava

原因:kryo序列化缓存空间不足。node
解决方案：调整参数 , --conf spark.kryoserializer.buffer.max=2047m

![img.png](../.vuepress/public/img/13.data_dev_sample_share/serialization_compare_001.png)

##### 2)、Error in query: nondeterministic expressions are only allowed in Project, Filter, Aggregate or Window, found

原因：若是是SparkSQL脚本，则rand()等函数不能出现join...on的后面

##### 3)、driver端日志中频繁出现：Application report for application_xxx_xxx (stage: ACCEPTED)

原因:没有资源可以使用
解决方案：经过yarn UI左侧的“Scheduler”界面，搜索本身任务提交的yarn队列，查看资源是否用完，与同队列同事协调资源的合理使用，优化资源使用量不合理的任务

##### 4)、ERROR SparkUI: Failed to bind SparkUI java.net.BindException: Address already in use: Service failed after 10 retries

原因:Spark UI端口绑定尝试连续10个端口都已被占用
解决方案: 调大参数。--conf spark.port.maxRetries=100。

##### 5)、Container killed by YARN for exceeding memory limits. 12.4 GB of 11GB physical memory used.

?103

原因:
1.数据倾斜，个别executor内存占用很是大超出限制。
2.任务小文件过多，且数据量较大，致使executor内存用光。
3.任务参数设置不合理，executor数量太少致使压力负载集中在较少的executor上。
4.代码不合理，有repartition(1)等代码逻辑。

解决方案:
1.若是确实是发生了数据倾斜，能够根据该连接的方法进行处理，也能够根据业务逻辑对关键字段加上distribute by语句进行哈希分发来缓解；若是是Spark3以上，或者公司平台的spark2.x源码中定制合入了社区的AQE特性，也能够加上这两个参数自动缓解：set spark.sql.adaptive.join.enabled=true和set spark.sql.adaptive.enabled=true。
2.能够优化代码将过多的连续join语句（超过5个）拆分，每3个左右的连续join语句结果生成一个临时表，该临时表再和后面两三个连续join组合，再生成下一个临时表，并在临时表中提早过滤没必要要的数据量，使多余数据不参与后续计算处理。只有在代码逻辑性能和参数合理的前提下，最后才能增长--executor-memory、--driver-memory等资源，不能本末倒置。
3.查看是否任务参数设置不合理，例如executor-memory是设的大，可是--num-executors设置的不多才几十个，能够根据集群状况和业务量大小合理增大executor数。
4.查看代码中是否有如repartition(1)等明显不合理的逻辑。
5.在代码性能与逻辑合理，且参数合理的前提下再增长资源，可增长对外内存：--conf  spark.yarn.executor.memoryOverhead=4096（单位为M，根据业务量状况具体设置）。

直接调节堆内内存，根据默认数值堆外内存可以自动调节

##### 6)、Table or view not found: user_login

这个错误很简单，就是表或者视图没找到。引起这个原因的可能如下：
1.如果你是连接hive中的数据表，程序有没有正确加载hive-site.xml。在你的代码resource里面加上hive-site.xml或者在spark-submit提交的时候–files指定hive-site.xml目录（如果是采用–files加载hive配置，spark on yarn hive-site.xml最好放在hdfs上，如果放在本地目录，则需要在所有的nodemanager上都创建hive-site.xml目录）或者在安装的spark目录下的conf中将hive-site.xml拷贝过去，以上3种方案完成一种即可。
2.创建sparkSession的时候，没有加enableHiveSupport()这个选项。SparkSession.builder().appName(“HiveMySQLApp”).master(“local[2]”).enableHiveSupport().getOrCreate()
3.可能是你hive当中或者spark sql当中真的没存在那个表格。这里可能要细心下，仔细检查下库名表名，查看hive当中是否存在该表。或者spark sql中是否存在该视图
4.是否正确使用了spark的临时视图和全局视图，如果你跨session使用了临时视图而不是全局视图，也会出现这个问题。

##### 7)、Permission denied: user=aaa_community, access=READ_EXECUTE, inode="/warehouse/aaa/ods/user_data/":hive:hive:drwxrwx–x

一般就是某个目录下的写权限是某个用户没有的，只需要找集群管理者或者管理员给你适配下权限即可

##### 8)、It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running ‘REFRESH TABLE tableName’ command in SQL or by recreating the Dataset

解决方案:
当你更新了一个表的某些列，并且马上查询的话，就会报这种错误。如果想解决这个问题，请刷新与该表关联的所有缓存项。调用命令如下：
REFRESH TABLE [db_name.]table_name

##### 9)、Not enough memory to build and broadcast the table to all worker nodes. As a workaround, you can either disable broadcast by setting spark.sql.autoBroadcastJoinThreshold to -1 or increase the spark driver memory by setting spark.driver.memory to a higher value

解决方案:
spark.sql.autoBroadcastJoinThreshold 单位字节，如果小于这个数字，则spark采用广播变量的方式进行join，默认是10M(10485760)。如果出现此问题，可能就是driver或者executor的内存溢出了。
解决办法有三种：1. spark.sql.autoBroadcastJoinThreshold 设置为-1关闭掉大表join小表的功能。 2. 将driver和executor申请的内存够调大 3. 将spark.sql.autoBroadcastJoinThreshold 数值调小。

##### 10)、User did not initialize spark context!

解决方案:将代码中设置master的方法去掉 .master(sparkMaster) 不要出现这句。

##### 11)、Dynamic partition strict mode requires at least one static partition column. To turn this off set hive.exec.dynamic.partition.mode=nonstrict

解决方案:
设置这个参数就行set hive.exec.dynamic.partition.mode=nonstrict; 默认是严格模式，严格模式的意思就是至少要有一个静态分区的存在，如果你的插入数据全部都是动态分区，就要加上上述的配置。

##### 12)、ValueError: RDD is empty

原因:当时同take(1) 或者 first()方法的时候，如果rdd的数据内容为空，会报此种异常。
   def first(self):
        rs = self.take(1)
        if rs:
            return rs[0]
        raise ValueError("RDD is empty")

##### 13)、ERROR scheduler.TaskSetManager: Total size of serialized results of 11870 tasks (1024.0 MB) is bigger than spark.driver.maxResultSize (1024.0 MB)

解决方案:spark.driver.maxResultSize默认大小为1G 每个Spark action(如collect)所有分区的序列化结果的总大小限制，简而言之就是executor给driver返回的结果过大，报这个错说明需要提高这个值或者避免使用类似的方法，比如countByValue，countByKey等。将值调大即可, --conf spark.driver.maxResultSize=2g

##### 14)、java.lang.ClassNotFoundException: XXX

原因：一般可能是用户jar和Spark jar冲突
解决方法：1、最好和Spark相关的jar进行适配。2、如果不行可以使用参数：spark.driver.userClassPathFirst和spark.executor.userClassPathFirst 设置为true

##### 15)、进行shuffle抛出：Shuffle Fetch Failed: OOM

原因：Shuffle fetch阶段开启的fetch数据量过大导致

解决方法：1、加大Executor内存。2、将参数spark.reduce.maxSizeInFlight调小，默认48M





## SQL Join执行的常见问题及解决方案

#### 1、string和bigint做join，可能导致不相等的关联上、出现重复数据、分区裁剪失效

这个问题源于底层的隐式转换规则，当string和bigint做比较的时候，会将两侧分别转换为double类型，而这个转换可能存在一定的精度损失，导致出现本来不相等的两个值能够join上

如果恰好是例如udt的分区字段，隐式转换后的join不会触发分区裁剪，扫描全表数据

```sql
t1.id = "111111111111111111" ;
t2.id = 111111111111111110 ;

select 
	a.id, b.id
from a join b
on a.id = b.id;



因为上面的sql转化为了
select 
	a.id, b.id
from a join b
on cast(a.id as double) = cast(b.id as double);

解决办法：
select 
	a.id, b.id
from a join b
on a.id = cast(b.id as string);
```

![img.png](../.vuepress/public/img/13.data_dev_sample_share/sql_sample_001.png)



#### 2、**过滤条件位置不对导致outer join退化为inner join**



#### 3、**多路join,join on 多列，有相同的前缀列**

```sql
问题1：
select 
	a.*, b.v1, c.v2
from a 
left join b on a.k1 = b.k1 and a.k2 = b.k2
left join c on a.k1 = c.k1;

尽可能使用前一个join已经shuffle好的结果，通过一个局部的排序就避免第二步产生额外的shuffle
select 
	a.*, b.v1, c.v2
from a 
left join c on a.k1 = c.k1
left join b on a.k1 = b.k1 and a.k2 = b.k2;



问题2：
select 
	a.*, b.v1, c.v2
from a 
left join c on a.k2 = c.k2 and a.k1 = c.k1
left join b on a.k1 = b.k1 and a.k2 = b.k2;


问题3：
现在要用a表关联4个维表
如果b表可以过滤a表的数据，c不会产生膨胀，d表会产生膨胀，则一个比较理想的join顺序？


select 
	a.k ak, b.v1, c.v2, d.v3
from a

join b on a.k1 = b.k1 --b表可以过滤a表的部分数据

left outer join c on a.k2 = c.k2--c不会导致数据膨胀

left outer join d on a.k3 = d.k3 --d会产生数据膨胀
```



#### 案例

两个表进行join，分布式a left join b ,两表信息如下：

表a： dws_device_pkg_install_status_180d_df

​			存储格式ORC,分区20210530 总行数500亿行，资产上看分区存储大小单副本500G.

表B：XXX.dws_device_pkg_install_status_180d_df

**结果表的总条数没变都是500亿行，字段个数也没变，字段也没做过啥处理，但是结果表的总存储变变成了3TB(单副本)**

```sql
INSERT overwrite TABLE xxx.dws_device_pkg_install_status_180d_df partition(DAY='20221122')
SELECT a.device,
       a.pkg,
       install_datetime,
       unstall_datetime,
       all_datetime,
       df_final_time,
       final_flag,
       final_time,
       coalesce(b.refine_final_flag, a.reserved_flag) as reserved_flag,
       process_time from
  ( SELECT device, pkg, install_datetime, unstall_datetime, all_datetime, df_final_time, final_flag, final_time, reserved_flag, process_time
   FROM dws_device_pkg_install_status_180d_df
   WHERE DAY = '20210530' ) a
LEFT JOIN  
(select device,pkg,refine_final_flag
   from  xxx.dws_device_pkg_install_status_180d_df_md  group by device,pkg,refine_final_flag )  b 
ON a.device = b.device and a.pkg = b.pkg
```



先试试纯map任务 **整体都在500G上下，跟之前3TB差别很大。**

```sql
INSERT overwrite TABLE xxx.dws_device_pkg_install_status_180d_df partition(DAY='20221122')
SELECT a.device,
       a.pkg,
       install_datetime,
       unstall_datetime,
       all_datetime,
       df_final_time,
       final_flag,
       final_time,
        reserved_flag,
       process_time from
  ( SELECT device, pkg, install_datetime, unstall_datetime, all_datetime, df_final_time, final_flag, final_time, reserved_flag, process_time
   FROM xxx.dws_device_pkg_install_status_180d_df
   WHERE DAY = '20210530' ) a
```



shuffle的问题，因为实际测试看，只有shuffle造成了数据膨胀，那就是问题处在shuffle上，shuffle的作用干啥？大家回忆一下，shuffle的过程其实就是对map输出的kv键值对，使用key按照hash算法进行分发到不同的reuduce上，shuffle过程基本就是：分区+排序+分发，reduce的过程就是合并排序+计算。

这么一看，有点思路了，应该大概率是排序造成的。因为排序对压缩的影响性能很大？

压缩原理其实很简单，就是找出那些重复出现的字符串，然后用更短的符号代替，从而达到缩短字符串的目的。

比如，有一篇文章大量使用"中华人民共和国"这个词语，我们用"中国"代替，就缩短了 5 个字符，如果用"华"代替，就缩短了 6 个字符。事实上，只要保证对应关系，可以用任意字符代替那些重复出现的字符串。本质上，所谓"压缩"就是找出文件内容的概率分布，将那些出现概率高的部分代替成更短的形式。所以内容越是重复的文件就可以压缩地越小。比如 ，“ABABABABABABAB"可以压缩成"7AB”。相应地，如果内容毫无重复，就很难压缩。



为了验证这个想法，看了下源表: dws_device_pkg_install_status_180d_df的生成业务逻辑，发现这个表是通过很多表加工而来，这个表生成的时候通过device进行shuffle的（这个表生成的十几张原表通过device-join聚合而来，所以其实数据是通过device进行hashshuffle而来的）。这个表的每个分区大概都是500亿行数据，分区大小单副本500G上下。而现在我们生成的结果是shuffle的条件是device+pkg连个复合键。

```sql
INSERT overwrite TABLE xxx.dws_device_pkg_install_status_180d_df partition(DAY='20221122')
SELECT a.device,
       a.pkg,
       install_datetime,
       unstall_datetime,
       all_datetime,
       df_final_time,
       final_flag,
       final_time,
       coalesce(b.refine_final_flag, a.reserved_flag) as reserved_flag,
       process_time from
  ( SELECT device, pkg, install_datetime, unstall_datetime, all_datetime, df_final_time, final_flag, final_time, reserved_flag, process_time
   FROM dws_device_pkg_install_status_180d_df
   WHERE DAY = '20210530' ) a
LEFT JOIN  
(select device,pkg,refine_final_flag
   from  xxx.dws_device_pkg_install_status_180d_df_md  group by device,pkg,refine_final_flag )  b 
ON a.device = b.device and a.pkg = b.pkg
cluster by a.device
```

因为这个表里device字段有大量的重复值，如果你用device进行shuffle的话，那么同样device，或者相近的device会被分配到同一个reduce里，这样最后orc列式存储时会有非常高的压缩率，重复值越高，压缩率越高。这个时候你换成device+pkg作为shuffle过程的hash-key，那么数据会打的更散，这个时候压缩率会有影响，造成跟之前相比会有膨胀。

