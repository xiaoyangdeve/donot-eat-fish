---
title: UBA-事件分析：全流量聚合模型
date: 2023-05-22 11:21:22
permalink: /pages/855b59/
categories:
  - 数据分析
tags:
  - 大数据
  - 行为分析
author: 
  name: 不爱吃鱼的bobo
---



## 1、背景

随着**数据爆炸、技术进步、商业竞争、个性化和客户体验、科学研究和发现**的发展和需要，形成了包括**数据采集、数据传输、数据加工（指标构建分析）、数据分析**等一整套的方法和工具，用来进行数据驱动。数据驱动是一种决策和问题解决方法，它基于对大量数据的分析和理解来指导决策过程。数据采集、埋点规划、数据建模、数据分析和指标体系构建。在用户行为数据领域，对多维数据模型进行信息提炼和模型整合，可以形成一套常见的数据分析方法来发现用户行为的内在联系，能更好洞察用户的行为习惯和行为规律，帮助企业挖掘用户数据的商业价值，提升产品的活跃用户数和核心功能使用深度、延长用的生命周期、提升产品使用体验和多途径变现能力增强。

在互联网行业内最早开始进行用户行为数据的工具建设可以以Google Analytics埋点分析工具为开始，在国内也有神策、GrowthingIO等商业化独立数据分析平台，在头部的互联网企业内也自主建设了各自的分析工具，如字节的火山引擎、美团的Ocean等。

### 1.1 行为分析（User Behavior Analysis, UBA）是什么

用户行为分析是指对用户在特定环境中的行为进行系统性的收集、分析和解释，以获取对用户行为模式、偏好和动机的深入了解。这种分析可以应用于各个领域，包括营销、用户体验设计、电子商务、社交媒体、商业化等。

通过用户行为分析，可以获得以下信息：

​		**用户行为模式**：分析用户在特定环境中的行为模式，如他们访问网站的路径、浏览页面的顺序、使用应用程序的方式等。这有助于了解用户的兴趣、需求和习惯。

​		**用户偏好**：通过分析用户的行为和选择，可以洞察他们对产品、服务或内容的偏好。这可以帮助企业定制个性化推荐、优化产品功能或调整营销策略。

​		**转化率分析**：用户行为分析可以帮助确定转化率，即用户从某个阶段转变为下一个阶段的比率。例如，在电子商务中，可以分析用户从浏览产品到实际购买的转化率，以识别销售漏斗的瓶颈并改进转化率。

​		**用户反应和参与度**：用户行为分析可以提供关于用户对特定活动或内容的反应和参与度的洞察。例如，在社交媒体中，可以分析用户对帖子的点赞、评论和分享行为，以了解用户对内容的兴趣和参与程度。

​		**用户满意度和忠诚度**：通过分析用户的行为和反馈，可以评估用户的满意度和忠诚度。这有助于企业改善产品、服务和用户体验，以提高用户保留率和口碑。

为了进行用户行为分析，通常会收集和分析多种类型的数据，包括网站或应用程序的日志数据、用户交互数据、调查数据、社交媒体数据等。分析方法可以包括统计分析、数据挖掘、机器学习和可视化等技术手段。

通过用户行为分析，组织可以更好地理解用户需求和行为模式，以优化产品和服务，并制定更有效的营销策略。

### 1.2 数据来源

用户行为分析的数据主要来源于各业务的打点数据，且基于事件模型进行建模，用户在产品中的各种操作都可以抽象成 Event 实体，并且里面都会包含五要素：

- **Who**：即参与这个事件的用户是谁，例如用户的唯一 ID；
- **When**：即这个事件发生的实际时间，例如`time`字段，记录精确到毫秒的事件发生时间；
- **Where**：即事件发生的地点，例如根据 IP 解析出的省份和城市；
- **How**：即用户从事这个事件的方式，例如用户的设备，使用的浏览器，使用的 App 版本等等；
- **What**：描述用户所做的这件事件的具体内容，例如点击类型的事件，需要记录的字段有点击 URL，点击 Title，点击位置等。

业内常见的分析模块有以下分析功能，比如神策数据分析模块：

<center><img src="/donot-eat-fish/img/data_analysis/uba/uba_model_sensor.png" width="80%" /></center>

## 2、技术历程

在行业内，关于用户行为分析的技术发展，主要可以分为三个阶段：**部分定制任务模型阶段、宽表大模型阶段、预聚合模型阶段**。

### 2.1 第一阶段：部分定制任务模型

这个阶段属于主要是根据用户选择和配置的模型参数（如漏斗分析），生成定制的Spark任务，然后将任务例行调度后，提供给前端分析工具查询。不同的分析模型或者相同分析的不同数据模型在后台对应不同的数据计算任务。

整个阶段的主要可以分为：**配置、计算、存储使用**三个阶段。

- 配置：主要是后端工程的服务实现，用户在前端根据自身的需求设置不同的分析模型、分析步骤、筛选参数、时间区间和周期等，后端收到服务的配置请求后，依据不同的任务组装器对任务进行组装，常见的可以组装为Spark任务。
- 计算：平台根据任务配置定时执行，计算结果将同步到MySQL或者ClickHouse中。
- 存储使用：结果集持久化道数据库中，通过分析工具服务提供给用户使用。

这个阶段主要解决了功能实现，但是也有明显的弊端：

- **不够灵活**：用户选择的配置数据需要提前配置和加工到模型表中，后期不易维护和调整，且同时只支持固化模型的查询，对于更多维度的数据分析保留不全；
- **资源浪费**：Spark任务在例行调度期间任务集中，需要向集群申请大量的资源，存在明显的波峰波谷现象。

### 2.2 第二阶段：大宽表模型+ClickHouse

ClickHouse是Yandex公司于2016年开源的一个列式数据库管理系统。Yandex的核心产品是搜索引擎，非常依赖流量和在线广告业务，因此ClickHouse极致的单表查询响应天生就适合用户流量分析。

1. 消费原始数据的消息队列，通过Flink或者Spark任务，将数据直接清洗后存入ClickHouse中，此时ClickHouse中的表为**无模型大宽表**；
2. 同时借助Redis等工具用来存储维度数据，通过ClickHouse的**RoaringBitMap**函数将行为数据和维度数据进行交并处理。

相比于第一个阶段，这个阶段解决了不够灵活的问题。但是这种方式是通过资源消耗为前提的，对集群和分析工具的资源需求非常大，所以缺点也很明显。

- **资源消耗大**：需要存储所有明细数据的展开，同时对Redis的维表关联性能要求很高。
- **数据一致性**：与离线数据仓库之间维护了两套逻辑，需要对两套逻辑的处理保持一致，否则会出现数据不一致情况。
- **元数据瓶颈**：该阶段的改造解决了集群头部业务大查询的问题，此时虽然独立集群存储没问题，但由于其他业务接入后还会持续增加数据量和埋点字段 ，这样会导致元数据进入瓶颈。可以通过事件和属性筛选映射后进行建表，来降低该风险。

<center><img src="/donot-eat-fish/img/data_analysis/uba/uba_model_stage_2.png" width="80%" /></center>

### 2.3 第三阶段：数据湖全量预聚合模型+ClickHouse

随着业务的变化，越来越多（十亿甚至百亿、千亿）的埋点数据上报到数据侧，这个时候的大宽表模型的查询速度瓶颈就越来越明显了，很多时候一个分析任务会需要特别长的时间才能得到结果（甚至会失败）。当这种特别大的任务hold住整个集群时，会使得其他的ad-hoc任务无法正常运行，严重的时候甚至会导致集群瘫痪等问题。

同时随着数据量的增大，对集群和资源的要求也越来越高，这个时候我们需要找到一种方法来进行有效的降本增效。这就要求我们要以最少的数据量更快的速度完成分析查询，直白来讲就是我们把数据量降下来。

那么，把数据量降下来的最简单方法我们就能够很容易想到了，那就是“**砍**”和“**聚**”。

- 砍：定期或者不定期通过对业务进行分析，移除掉不访问或者低频访问的数据，这种会造成被移除掉的数据将无法通过我们的分析工具进行分析使用了。
- 聚：通过全量预聚合模型，针对不同的分析模型，设计适用的通用模型，对数据通过不同的维度进行最细粒度的聚合，这种**需要牺牲一定的时间粒度**，因为需要要求将一定范围内的数据进行聚合，过细的世间会使得数据的聚合效果不明显或者查询性能负增长。

通过全量预聚合模型，我们可以将数据行数缩小至原有数据量的十分之一甚至百分之一。同时数据的产出底层数据链路通过Kappa架构实现，数据的产出可以做到小时级别甚至分钟级别（会产生小文件，对数据湖的性能要求提升）。再结合特定的分片分区方式配合下推参数，利用分区、主键等索引方式支持事件分析、漏斗分析、路径分析、留存分析等功能，同时提升数据的查询效能。

<center><img src="/donot-eat-fish/img/data_analysis/uba/uba_model_stage_3.png" width="80%" /></center>

这个阶段的UBA主要拥有一下特点：

- **全流量聚合模型**：可以认为是全量埋点数据的流量模型结构，除了将事件维度退化外，其他维度理论上都能保留，数据量能缩减至原有数据的百分之一到十分之一；
- **现查现算模式**：不再采用与计算模式，通过另一套基于CK的标签平台生成的用户人群可以实现跨集群关联计算，可以灵活指定想要分析的用户群体属性；
- **批量导出加速**：数据通过BulkLoad的方式出湖至ClickHouse，超大规模数据量快速到处；
- **维度服务升级**：借助snowflake等工具对维度字段的服务进行加强；
- **SQL引擎路由**：Iceberg数据湖中的数据作为CK的备用链路，可以承载复杂的业务分析功能，同时提升数据可用性保证；
- **湖仓一体思想**：用户行为的数据分析从传统的强离线的引擎过渡到强OLAP引擎，同时借助于大数据技术的不断发展和进步，其底层明细数据还可以采用hudi，可以满足更加实时的数据消费。

## 3、事件分析

事件分析，是**指基于事件的指标统计、属性分组、条件筛选等功能的查询分析**，本质上是分析埋点事件的用户触发情况以及埋点事件的分析统计情况，可以支持单个事件分析、多个事件的对比分析以及多个事件的复合指标运算。

可以帮助回答以下问题：最近三个月来自哪个渠道的用户注册量最高？变化趋势如何？各个时段的人均充值金额是分别多少？上周来自XX的，发生过购买行为的独立用户数，按照年龄段的分布情况？

我们现在的事件分析功能是基于大宽表模型的明细数据，其行为数据每天增量数十亿级别，存储日增TB级别以上资源消耗巨大，明细数据分析查询比较慢，所有的分析服务基于Trino的ad-hoc集群进行实现，每天用户的慢查询甚至可能超过10分钟，用的体验感极差，而且其功能比较单薄，只支持单个分析内的查询，不支持人群交并等复杂分析模块。

面临的问题主要有：

- 怎样提高用户的查询速度？
- 怎样与其他系统进行打通，如画像、A/B-Test等？
- 在满足复杂业务分析场景的时候，如何能够缩小数据量，压缩存储提升查询效率？
- 怎样在满足时效的同时还可以满足海量数据的计算需求？

<center><img src="/donot-eat-fish/img/data_analysis/uba/uba_event_analyse01.png" width="80%" /></center>

### 3.1 总体思路

1. 事件分析通过准实时方式建模分层，用户、事件、时间等粒度的预聚合压缩，不仅统一了离线口径，而且自研拉宽汇聚spark脚本可以承载百亿数据压力，搭配多种聚合模型实现丰富的分析模块；
2. 通过小时任务保证时效性，维表压力采用join离线维表+属性字典维度服务的方式解决；
3. 结合自研可指定shard的BulkLoad出仓工具，配合下推参数可加速查询，数据链路可扩展易运维。

相比较以往的处理千亿明细数据，准实时在OLAP层实现了对数据的压缩，将每天压缩至百分之一到十分之一，同时也通过汇总后的数据替代了原先的明细数据，**大大缩小存储的同时也提高了查询性能，时间窗口可扩大至更长**。并且对高复杂的查询比如用户留存，用户分群等分析场景可以更好的支持。

### 3.2 模型结构设计

**流量聚合模型建设方案**：

<center><img src="/donot-eat-fish/img/data_analysis/uba/uba_event_analyse03.png" width="80%" /></center>

1. 首先准实时清洗DWD层十亿百亿级明细行为数据，流量数据都是分为私有参数和公有参数，其中公有参数在用户粒度下是不会经常改变的，我们会用一般聚合函数取一定时间内指定设备和行为事件下最新保留的不变公有参数，而将同等粒度下变化比较频繁的私有参数维度名写入Array结构；
2. 利用map索引原理，把私参维度值组合通过spark自定义逻辑计数并入map的key中，map的value则用来写入各种公共指标聚合结果，整个过程均通过spark脚本实现，最终写入到Iceberg数据湖中；
3. 因为Iceberg可以关联其他任何已有hive或者iceberg表，通过快速业务表关联也可以支持到其他多项业务应用，也可以作为不出湖的降级备用方案支持大部分查询分析功能。

#### 3.2.1 如何使用

流量聚合模型可以在Iceberg和ClickHouse使用，在不**同引擎上查询时可以设计特定的UDF函数来解析嵌套结构，降低使用难度和避免多次查询遍历带来性能下降**。目前我们只是在Iceberg中使用，在使用的时候可以通过区分是否包含私参的维度聚合来选择不同的使用方式：

<center><img src="/donot-eat-fish/img/data_analysis/uba/uba_event_analyse04.png" width="80%" /></center>

- 无私参维度聚合：公参私参过滤 -> 计算pv、uv

```sql
with temp_data_1 as (
select
    extended_field_key_list
    ,reduce(
        map_values(
        -- 私有参数过滤
             map_filter(extended_field_val_map, (k, v) -> case when split(k, '`')[array_position(extended_field_key_list, 'ver')] not in ('xxx', 'xxxx') then true else false end
            )
        )
        , 0
        , (s, x) -> s + cast(x[1] as bigint)
        , s -> s
    ) as pv
    ,serial_num
    ,dc
    ,dt
    ,country
    ,province
    ,city
from olap.flow_agg_model_detail_v1
-- 分区筛选
where dt between '20230411' and '20230514'
-- 公共参数过滤
and dc in ('xxx', 'xxx') 
and dim1 = 1000 and type = 1
)
select
  coalesce(dt, '总体') as dt
  ,coalesce(dc, '总体') as dc
  ,coalesce(country, '总体') as  country
  ,coalesce(province, '总体') as province
  ,coalesce(city, '总体') as     city
  ,sum(pv) nums
  ,count(distinct serial_num) dv
from temp_data_1
GROUP BY GROUPING SETS (
()
, (dt)
, (dc,  country)
, (dc,  country, province)
, (dc,  country, province, city)
, (dt, country)
, (dt, country, province)
, (dt, country, province, city)
, (dt, dc, country)
, (dt, dc, country, province)
, (dt, dc, country, province, city)
)
order by dv desc,nums desc
Limit  100
```



- 有私参维度聚合：提取私参组合并合并merge指标值 -> 展开私参维度组合 -> 计算pv、uv，同时在有私参参与聚合的时候需要开发udf来配合解析复杂的map嵌套结构。

```sql
with temp_data_1 as (
select
    extended_field_key_list
    -- 私参预聚合, 从metric_map中抽取需要的私有参数字段维度组合，并将相同值的key指标进行sum
    -- todo:避免多次循环
    ,flow_model_private_prop_agg(array[array_position(extended_field_key_list, 'col1') - 1, array_position(extended_field_key_list, 'col2') - 1, array_position(extended_field_key_list, 'col3') - 1]
        -- 私参过滤
    ,map_filter(extended_field_val_map
                , (k, v) -> 
                case when split(k, '`')[array_position(extended_field_key_list, 'ver')] not in ('xxx', 'xxxx') 
                then true else false end	
    ) a s pre_extended_field_val_map
    ,serial_num
    ,dc
    ,dt
from olap.flow_agg_model_detail_v1
-- 分区筛选
where dt between '20230411' and '20230514'
-- 公共参数过滤
and dc in ('xxx', 'xxx') 
and dim1 = 1000 and type = 1
)
, temp_data_2 as (
select
    dt
    ,dc
    ,serial_num
    -- todo 避免多次split
    ,split(k, '`')[1] as col1
    ,split(k, '`')[2] as col2
    ,split(k, '`')[3] as col3
    ,val[1] as pv
from temp_data_1
cross join unnest(pre_extended_field_val_map) as t (k, val)
)
select
  coalesce(dt, '总体') as dt
  ,coalesce(dc, '总体') as dc
  
  ,coalesce(col1, '总体') as col1
  ,coalesce(col2, '总体') as col2
  ,coalesce(col3, '总体') as col3
  ,sum(pv) nums
  ,count(distinct serial_num) dv
FROM temp_data_2
GROUP BY GROUPING SETS (
()
, (dt)
, (dc,  col1)
, (dc,  col1, col2)
, (dc,  col1, col2, col3)
, (dt, col1)
, (dt col1, col2)
, (dc, col1, col2, col3)
, (dt, dc, col1)
, (dt, dc, col1, col2)
, (dt, dc col1, col2, col3)
)
order by dv desc,nums desc
limit 100
```

### 3.3 Trino查询性能对比

针对以上的两种使用方式，我在Trino集群上选取了流量比较大的曝光埋点同业务展开表的查询速度进行了测试。主要结果如下（同一查询多次测试，两种查询SQL连续提交）：

<center><img src="/donot-eat-fish/img/data_analysis/uba/uba_event_analyse05.png" width="80%" /></center>

## 4、ClickHouse优化方向

众所周知的是ClickHouse集群是一种存算一体的架构设计，这种设计下ClickHouse无法做到真正的弹性扩缩容，而UBA场景下的分析需要使用到近90天甚至更久的历史数据，所以对于ClickHouse的存储有着极大的要求。随着业务的增加，数据量持续增加，需要**ClickHouse去解决数据重平衡（Rebalance）**的问题。

如果后续采用ClickHouse等其他MPP架构类型存储数据库来进行OLAP服务的提供，对于查询性能可以有下面优化，此处以ClickHouse为例。

### 4.1 查询下推

ClickHouse中的**针对分布式表的查询会被改写成对local表的查询并发送到集群各个shard执行，然后将各个shard的中间计算结果收集到查询节点做合并**。当中间计算结果很大时，比如countDistinct、 windowFunnel函数等，查询节点的数据收集和数据合并可能成为整个查询的性能瓶颈。

查询下推的思路就是尽量将计算都下推到各个shard执行（需要提前做好shard），查询节点仅收集合并少量的最终计算结果。不过，也不是所有查询都适合做下推优化，满足以下两个条件的查询可以考虑做下推优化：

- 数据已经按照计算需求做好sharding：比如，UBA场景的数据已按one id做好了sharding，所以针对用户的漏斗分析，UV等计算可以下推到各个shard执行。否则，下推后的计算结果是不准确的。

- 计算的中间结果较大，比如像sum，count等计算是无需下推的，因为其中间结果很小，合并计算很简单，下推并不能带来性能提升，甚至会下降。 

### 4.2 索引支持

在关系数据库中，索引是一种单独的、物理的对数据库表中一列或多列的值进行排序的一种存储结构，它是某个表中一列或若干列值的集合和相应的指向表中物理标识这些值的数据页的逻辑指针清单。简单来说，索引的出现其实就是为了提高数据查询的效率，就像书的目录一样。

索引的实现方式有很多中，常见的主要是：哈希表、有序数组、搜索树、跳表、BloonFilter。

在UBA场景中的事件数据有很多公共属性和私有属性，我们选择了部分属性被设计为表的固定字段，而其他属性因为各个事件不尽相同，被定义成了私有属性，所以采用Array/Map来存储（也可以采用两个数组分别存储属性名和属性值）。无论是Array还是Map，最初都不支持创建跳数索引，所以在其他索引字段过滤效果有限的情况下，针对Array和Map的操作可能会成为查询的性能瓶颈，特别是Map比较大的情况下。

这个时候我们可以考虑给**Array和Map加上了Bloom filter等跳数索引支持**，针对Map仅对其key构建索引。在某些出现频率较低的私有属性过滤场景下，Array/Map的跳数索引可以收获数倍的性能提升。

### 4.3 存储优化

ClickHouse本身支持多宗方式的数据压缩，常用的数据压缩方式有三种，分别为**LZ4（默认）、ZSTD**。针对不同的数据类型，数据分布方式来使用特定的编码方式可以大大提高数据压缩率，以减少存储成本。

我们可以根据查询效率和写入时效的平衡，选择我们需要的压缩算法。

#### 4.3.1 通用压缩算法

- None：无压缩

- LZ4：默认的压缩算法,缺省值也是使用默认的压缩算法

- LZ4HC[(level)]：z4高压缩率压缩算法版本, level默认值为9,支持[1~12],推荐选用[4~9]

- ZSTD[(level)]：zstd压缩算法，level默认值为1，支持[1~22]

#### 4.3.2 特殊编码

- LowCardinality：枚举值数量较小的字符串，比如1000
- Delta：时间序列类型的数据，不会对数据进行压缩
- T64：比较适合Int类型数据
- DoubleDelta：适用缓慢变化的序列:比如时间序列，对于递增序列效果很好
- Gorilla：使用缓慢变化的数值类型

特殊编码与通用的压缩算法相比，区别在于，**通用的LZ4和ZSTD压缩算法是普适行的，不关心数据的分布特点，而特殊编码类型对于特定场景下的数据会有更好的压缩效果**。压缩算法和特殊编码两者可以结合起来一起使用。

比如：

```
CREATE TABLE ck.codec_test (
    `biz_time` Uint32,
    `biz_timet` UInt32 CODEC ( NONE ),
    `biz_time2` UInt32,
    `biz_time3` Ulnt32 CODEC ( LZ4 ),
    `biz_time4` UInt32 CODEC (LZ4HC ( 9 )),
    `biz_time5` UInt32 CODEC (ZSTD ( 9 ),
    `biz_time6` Ulnt32 CODEC ( T640 ),
    `name0` String CODEC (Delta (, LZ4 ),
    `name1` String CODEC ( DoubleDelta0 )),
    `name2` String CODEC ( Gorilla ( 0 ), 
    `name4` String cODEC ( Gorilla ), Lz4 ) 
    ) ENGINE = MergeTree () PARTITION BYtoYYYYMMDD ( toDateTime(biz_time )) 
ORDER BY
    biz_time
```

### 4.4 写入优化

从数据湖里将数据写出道ClickHouse中最常见的方案就是通过JDBC的形式写入。

如果**更高时效要求的同时需要更高速度的写入**，可以根据情况开发相应的BulkLoad工具。业界常见的BulkLoad工具主要有以下两种方案。

#### 4.4.1 文件缓存批量写入

<center><img src="/donot-eat-fish/img/data_analysis/uba/uba_ck_optimize01.png" width="80%" /></center>

1. 首先，将clickhouse格式的data part文件的生成过程转移到Spark Application中完成，这样就可以利用Yarn集群的资源来完成数据排序，索引生成，数据压缩等步骤。data part文件的生成我们借助clickhouse-local工具实现，在Spark Executor中调用clickhouse-local写入数据到本地磁盘，生成clickhouse data part文件。
2. 然后，将Spark Executor生成的data part文件上传到HDFS等文件系统的特定目录中。
3. 接着，从Spark Executor端发送 "ALTER TABLE ... FETCH PART/PARTITION" SQL语句到clickhouse server执行。
4. 最后，ClickHouse Server执行 "ALTER TABLE ... FETCH PART/PARTITION"，从HDFS等文件系统拉取data part文件并完成操作。其中，**可能需要对ClickHouse代码做一些改造**，使得FETCH语句支持从HDFS等存储拉取文件。

由于Bulkload导入将数据写入data part文件这个过程移到了Spark端执行，大大降低了ClickHouse Server数据写入对资源的消耗。与此同时，由于在Spark端数据批量写入之前已经完成了repartition和攒批，到达ClickHouse Server的data part数量相较JDBC写入要少很多，所以clickhouse的merge压力也大幅降低。

但这个方案依然存在一些问题：

1. 以HDFS等文件系统作为文件传输的中间存储，增加了数据传输的耗时和网络开销，同时会占用对应文件系统的存储资源。
2. HDFS等文件系统的负载情况可能影响ClickHouse Bulkload数据导入的性能与稳定性。

#### 4.4.2 直接批量ClickHouse

模拟ClickHouse副本间数据同步的DataExchange服务，开发对应程序，在Spark、Flink等应用程序中将data part文件直接传输到Click Server，同时完成鉴权、校验、流量控制、并发控制等操作。

<center><img src="/donot-eat-fish/img/data_analysis/uba/uba_ck_optimize02.png" width="80%" /></center>

## 5、接下来工作

- 漏斗分析模型和路径分析模型及更多业务场景模型支持
- Trino更深层次udf和源码修改，支持更快查询
- 使用ClickHouse等MPP数据库
- Z-Order索引支持

## 参考

https://www.bilibili.com/read/cv21106590?spm_id_from=333.999.0.0

https://www.cnblogs.com/MrYang-11-GetKnow/p/16016566.html

https://www.sensorsdata.cn/product/analysis.html
